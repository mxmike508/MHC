{
  "name": "Master Workflow",
  "nodes": [
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT *\nFROM conversation_history\nWHERE\n  session_id = $1\n  AND id > $2;",
        "options": {
          "queryReplacement": "={{ [ $('Trigger: Receive Session ID').first().json.body.chat_session_id, $('Set: Watermark Value').first().json.highWaterMark ] }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -768,
        -1280
      ],
      "id": "b230a12d-38da-4990-8236-2b26309127e8",
      "name": "DB: Get Conversation History",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "const items = $('DB: Get Conversation History').all();\nlet assembledTranscript = \"\";\n\nfor (const item of items) {\n  // Only process items that have a role and content\n  if (item.json.role && item.json.content) {\n    const role = item.json.role;\n    const content = item.json.content;\n    const formattedRole = role.charAt(0).toUpperCase() + role.slice(1);\n    assembledTranscript += `${formattedRole}: ${content}\\n`;\n  }\n}\n\n// If empty, return benign output; downstream can decide to no-op.\nif (assembledTranscript.trim() === \"\") {\n  return [{ json: { assembledTranscript: \"\" } }];\n}\n\nreturn [{ json: { assembledTranscript: assembledTranscript.trim() } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -96,
        -1280
      ],
      "id": "ecc3997f-f6e6-4d9d-9d64-8c3cfb02b8ca",
      "name": "Format: Assemble Transcript",
      "alwaysOutputData": false
    },
    {
      "parameters": {
        "jsCode": "// Get the transcript from the previous node\nconst transcript = $input.item.json.assembledTranscript;\n\n// --- CORRECTED PROMPT ---\n// This new prompt is more lenient and will capture more information.\nconst system_prompt = `You are a meticulous data extraction engine. Your sole purpose is to convert a conversation transcript into a structured JSON object containing a list of key informational statements.\n\nRules:\n- Your output MUST be a JSON object with a single key: \"facts\".\n- The \"facts\" key must contain an array of strings.\n- Each string should be a key piece of information, a specific detail, or a factual statement.\n- Extract informational content from both the User and the Assistant.\n- IGNORE simple pleasantries (hello, thanks) and questions that do not contain information.\n- If no new information is found, the \"facts\" array MUST be empty: [].`;\n\n// Build the complete JSON body for the OpenAI API call\nconst requestBody = {\n  \"model\": \"gpt-4o\",\n  \"response_format\": { \"type\": \"json_object\" },\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": system_prompt\n    },\n    {\n      \"role\": \"user\",\n      \"content\": transcript\n    }\n  ]\n};\n\n// Return the entire object for the next node\nreturn requestBody;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        128,
        -1280
      ],
      "id": "8b3fef8e-f2e4-4105-ba34-1d0dfc1922c8",
      "name": "Build: OpenAI Request Body"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "contentType": "raw",
        "rawContentType": "=application/json",
        "body": "={{ JSON.stringify($json) }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        352,
        -1280
      ],
      "id": "1871d369-6d6e-45d6-a295-2a43229c94e0",
      "name": "Execute: OpenAI API Call",
      "credentials": {
        "openAiApi": {
          "id": "XWtCAAEVWi0CPB0c",
          "name": "OpenAi (Chat Completions)"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT COALESCE(\n  (SELECT last_processed_message_id FROM memory_commit_log WHERE session_id = $1),\n  0\n) as high_water_mark;",
        "options": {
          "queryReplacement": "={{ $('Trigger: Receive Session ID').first().json.body.chat_session_id }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -1200,
        -1280
      ],
      "id": "0d3090b2-2c46-49c7-992f-6a299d8685c1",
      "name": "DB: Get High-Water Mark",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "28e6b89f-2ac1-420d-9774-231e6d7834c3",
              "name": "highWaterMark",
              "value": "={{ $json.high_water_mark ?? 0 }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -976,
        -1280
      ],
      "id": "80ff13c1-040d-4dde-80fd-7c29269d7ac7",
      "name": "Set: Watermark Value"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "b2c6aead-ea59-4a6e-9101-e1f9541c8d8e",
              "name": "summary_text",
              "value": "={{ $json.choices[0].message.content }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        560,
        -1280
      ],
      "id": "3bb08844-61cd-4a73-be51-0cd2c2bf9646",
      "name": "Set: Extracted Summary"
    },
    {
      "parameters": {
        "jsCode": "// Count number of distilled facts for commit status / toast.\nconst s = $('Set: Extracted Summary').first().json.summary_text;\nlet count = 0;\ntry { const obj = JSON.parse(s); if (Array.isArray(obj.facts)) count = obj.facts.length; } catch (e) {}\nreturn { count };"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1664,
        -1184
      ],
      "id": "2aa617e7-f573-4b22-84dd-4673eebb052c",
      "name": "Count Facts"
    },
    {
      "parameters": {
        "jsCode": "// This node takes the JSON string from the AI, extracts the facts,\n// and returns each fact as a separate item to be embedded.\n\n// Get the summary_text which contains a JSON string of facts.\nconst summaryJsonString = $input.item.json.summary_text;\n\n// Parse the string to get the actual JSON object.\nconst summaryObject = JSON.parse(summaryJsonString);\n\n// Get the array of facts from the object.\nconst factsArray = summaryObject.facts;\n\n// If there are no facts, return an empty array to stop the workflow.\nif (factsArray.length === 0) {\n  return [];\n}\n\n// Return each fact as a separate item, with the correct key.\nreturn factsArray.map(fact => {\n  return {\n    json: {\n      \"chunk_text_for_embedding\": fact\n    }\n  }\n});"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        784,
        -1392
      ],
      "id": "1a45e370-2cab-41a6-ac8b-92dda69e177e",
      "name": "Chunk: Distilled Summary"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO rag_store (project_id, session_id, original_content, role, embedding)\nVALUES ($1, $2, $3, $4, '{{ JSON.stringify($json.embedding) }}');",
        "options": {
          "queryReplacement": "={{ $json.project_id }},{{ $json.session_id }},{{ $json.original_content }},{{ $json.role }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1440,
        -1392
      ],
      "id": "22ee9c26-971d-4583-a6f0-a8c307836848",
      "name": "DB: Insert Embedding1",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO memory_commit_log (session_id, last_processed_message_id, processed_at)\nVALUES ($1, $2, NOW())\nON CONFLICT (session_id)\nDO UPDATE SET\n    last_processed_message_id = EXCLUDED.last_processed_message_id,\n    processed_at = NOW();",
        "options": {
          "queryReplacement": "={{ $('Trigger: Receive Session ID').first().json.body.chat_session_id }},{{ $('Calculate New High-Water Mark').first().json.new_high_water_mark }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1664,
        -1392
      ],
      "id": "e6a86b6a-499d-4d7b-a41f-818d67d2a1b9",
      "name": "DB: Update High-Water Mark",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "71077881-f2db-46b6-8537-c6fece3d5296",
              "leftValue": "",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "equals",
                "name": "filter.operator.equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -320,
        -1280
      ],
      "id": "83ee7f7b-ff7c-4d28-ad57-31862effe632",
      "name": "IF: New Messages Found?"
    },
    {
      "parameters": {
        "jsCode": "// This node finds the highest 'id' from the incoming messages.\nconst items = $input.all();\nlet maxId = 0;\n\n// If there are no items, the watermark is 0.\nif (items.length === 0) {\n  return [{ json: { new_high_water_mark: maxId } }];\n}\n\n// Find the highest id in the list of messages.\nfor (const item of items) {\n  if (item.json.id > maxId) {\n    maxId = item.json.id;\n  }\n}\n\n// Return the single highest value.\nreturn [{ json: { new_high_water_mark: maxId } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -544,
        -1280
      ],
      "id": "da6fb286-a566-4e75-baf3-ace5c35e26ff",
      "name": "Calculate New High-Water Mark"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/embeddings",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={ \"model\": \"text-embedding-3-small\", \"input\": \"{{ $json.chunk_text_for_embedding }}\" }",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        1008,
        -1392
      ],
      "id": "7b75af31-2a38-412f-99bd-9b1e3b96f590",
      "name": "API: Create Embeddings (HTTP) node",
      "credentials": {
        "openAiApi": {
          "id": "XWtCAAEVWi0CPB0c",
          "name": "OpenAi (Chat Completions)"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// This is the corrected final assembly. It pulls data from all necessary\n// previous nodes to create the complete payload for the database.\n\n// 1. Get the new embedding.\nconst embedding = $json.data[0].embedding;\n\n// 2. Get the summary text.\nconst summaryData = $('Set: Extracted Summary').first().json;\nconst summaryText = summaryData.summary_text;\n\n// 3. Get the project ID.\nconst projectData = $('Get Project ID').first().json;\nconst projectId = projectData.project_id;\n\n// 4. Get the RAG session ID to scope long-term memory writes.\nconst triggerData = $('Trigger: Receive Session ID').first().json;\nconst sessionId = triggerData.body.rag_session_id;\n\n// 5. Get the new high-water mark we just calculated.\nconst watermarkData = $('Calculate New High-Water Mark').first().json;\nconst newHighWaterMark = watermarkData.new_high_water_mark;\n\n// 6. Build the final, complete payload with EVERYTHING needed.\nconst finalPayload = {\n  project_id: projectId,\n  session_id: sessionId,\n  original_content: summaryText,\n  role: 'summary',\n  embedding: embedding,\n  new_high_water_mark: newHighWaterMark\n};\n\nreturn finalPayload;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1232,
        -1392
      ],
      "id": "b46126b4-f0a6-48b3-aa2d-ed9b2af2ef18",
      "name": "Merge Embedding with Original Data"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT xata_id AS project_id FROM project_contexts WHERE chat_session_id = $1",
        "options": {
          "queryReplacement": "=[   {{ $('Trigger: Receive Session ID').first().json.body.chat_session_id }} ]"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -1424,
        -1280
      ],
      "id": "bf062b5d-98c6-4fd4-a824-6130c9102282",
      "name": "Get Project ID",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "6c1ce608-2f7a-457b-9afc-f0be5ef4bd4c",
        "responseMode": "responseNode",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              },
              {
                "name": "Access-Control-Allow-Methods",
                "value": "POST, OPTIONS"
              },
              {
                "name": "Access-Control-Allow-Headers",
                "value": "Content-Type, Authorization"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -1648,
        -1280
      ],
      "id": "b8bcd531-9618-473a-8f68-880ad5a1d625",
      "name": "Trigger: Receive Session ID",
      "webhookId": "6c1ce608-2f7a-457b-9afc-f0be5ef4bd4c",
      "options": {
        "allowedOrigins": "*",
        "responseHeaders": {
          "entries": [
            {
              "name": "Access-Control-Allow-Origin",
              "value": "*"
            },
            {
              "name": "Access-Control-Allow-Methods",
              "value": "POST, OPTIONS"
            },
            {
              "name": "Access-Control-Allow-Headers",
              "value": "Content-Type, Authorization"
            }
          ]
        }
      }
    },
    {
      "parameters": {
        "content": "## System Documentation: Context Distillation & Indexing\n\n\nWorkflow Name: System: Context Distillation & Indexing for Documentation\nSource File: System__Context_Distillation___Indexing_for_Documentation.json\nProject Lead: Mike Holland\nSystem Architect: Gemini Pro\n1. Overall Goal\nThis workflow serves as the dedicated \"Commit to Memory\" function for the Chat 8 UI. Its purpose is to solve the problem of limited context windows in LLMs by creating a persistent, long-term memory store. It is triggered manually by the user, reads the recent, unprocessed conversation history for a given session, uses a powerful AI model to distill that conversation into a set of atomic facts, generates vector embeddings for those facts, and saves them to the rag_store database.\n2. Key Components & Architecture\nTrigger: A unique n8n Webhook (...6c1ce608...) listens for POST requests from the Chat 8 UI's \"Commit to Memory\" button.\nDatabase: It interacts with three key tables in the Postgres database: conversation_history (to read new messages), memory_commit_log (to track progress with a high-water mark), and rag_store (to save the final indexed memories).\nExternal Services: It makes two distinct calls to the OpenAI API: one to the Chat Completions endpoint for fact distillation and another to the Embeddings endpoint for vectorization.\nLogic: The workflow is a sequential data processing pipeline that includes a crucial \"high-water mark\" system to prevent duplicate processing of messages.\n3. Step-by-Step Data Flow\nThe workflow executes in a clear, linear sequence:\nTrigger: Receive Session ID: The workflow is triggered by a POST request from the UI. The payload contains the chat_session_id and rag_session_id for the current conversation.\nGet Project ID: A Postgres node takes the chat_session_id and queries the project_contexts table to find the associated permanent project_id.\nDB: Get High-Water Mark: A Postgres node queries the memory_commit_log table to find the ID of the last message that was processed for this chat_session_id. It uses a COALESCE function to safely default to 0 if no record exists (i.e., this is the first commit for the session).\nSet: Watermark Value: A Set node takes the high_water_mark from the previous step and prepares it for the next query.\nDB: Get Conversation History: A Postgres node queries the conversation_history table, fetching all messages for the current chat_session_id whose id is greater than the highWaterMark.\nCalculate New High-Water Mark: A Code node iterates through the messages retrieved in the previous step and finds the highest message id. This value will be saved at the end of the workflow.\nIF: New Messages Found?: An If node checks if any new messages were returned. If the count is zero, the workflow stops cleanly. If new messages exist, it proceeds.\nFormat: Assemble Transcript: A Code node takes the new messages and formats them into a clean, human-readable transcript string.\nBuild: OpenAI Request Body: A Code node constructs the full JSON payload for the fact-distillation call to the OpenAI API, including a detailed system prompt and the conversation transcript.\nExecute: OpenAI API Call: An HTTPRequest node sends the payload to the OpenAI Chat Completions API (gpt-4o).\nSet: Extracted Summary: A Set node parses the AI's response and extracts the JSON string containing the distilled facts.\nChunk: Distilled Summary: A Code node parses the JSON string of facts and splits them, outputting each individual fact as a separate item to be processed in the following steps.\nAPI: Create Embeddings (HTTP) node: (Runs for each fact) An HTTPRequest node takes each fact and calls the OpenAI Embeddings API (text-embedding-3-small) to generate its vector embedding.\nMerge Embedding with Original Data: A Code node (not a Merge node) runs to assemble the final, complete payload for the database, combining the project_id, session_id, the fact text, the role (summary), and the new embedding vector.\nDB: Insert Embedding1: (Runs for each fact) A Postgres node executes the INSERT query to save the complete record for each fact into the rag_store table. It correctly links each memory to the project_id and rag_session_id that were generated in Step 2.\nDB: Update High-Water Mark: The final database operation. A Postgres node performs an UPSERT on the memory_commit_log table, saving the new_high_water_mark calculated in Step 6. This ensures the next run will only process messages created after this point.\nCount Facts & Respond to Webhook6: A final Code node counts the number of facts committed, and the Respond to Webhook node sends a detailed success message (including a \"toast\" for the UI) back to the user, completing the cycle.",
        "height": 80,
        "width": 660
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -240,
        -1472
      ],
      "id": "fc517850-d01f-409a-b175-17acb9f9ac2b",
      "name": "Sticky Note3"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ ({\n  status: 'success',\n  toast: `Committed ${$('Count Facts').first().json.count ?? 0} fact(s) to memory.`,\n  status_line: `Memory updated: project_id=${$('Get Project ID').first().json.project_id}, rag_session_id=${$('Trigger: Receive Session ID').first().json.body.rag_session_id}, new_chunks=${$('Count Facts').first().json.count ?? 0}.`,\n  chunks_committed: $('Count Facts').first().json.count ?? 0,\n  project_id: $('Get Project ID').first().json.project_id,\n  rag_session_id: $('Trigger: Receive Session ID').first().json.body.rag_session_id\n}) }}",
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        2112,
        -1280
      ],
      "id": "55c53604-c7a5-4f68-8f3b-ee97ff448818",
      "name": "Respond to Webhook6"
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineAll",
        "options": {}
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        1888,
        -1280
      ],
      "id": "475e1e74-b1a4-4305-89c2-ac2d981a5bcc",
      "name": "Merge2"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO project_contexts (project_name, status, chat_session_id, rag_session_id)\nVALUES ($1, 'New', $2, $3)\nRETURNING xata_id AS project_id, chat_session_id AS thread_id;",
        "options": {
          "queryReplacement": "={{ [ $json.projectName, $json.chat_session_id, $json.rag_session_id ] }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -304,
        -656
      ],
      "id": "04848582-b13f-4fc2-a610-132fbfafa1f9",
      "name": "Postgres1",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT \n    project_name, \n    chat_session_id, \n    rag_session_id, \n    xata_id \nFROM \n    project_contexts \nORDER BY \n    xata_createdat DESC;",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -960,
        -768
      ],
      "id": "38ded463-acf2-4a33-832e-ba548951dbaa",
      "name": "Get Project List",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// This node receives the new project name and generates unique IDs.\n\nconst body = $json.body;\n\n// 1. Validate the input from the frontend.\nif (!body.projectName || typeof body.projectName !== 'string') {\n  throw new Error(\"The 'projectName' field is missing or is not a string in the webhook body.\");\n}\n\n// 2. Generate unique IDs for the chat session and the RAG session.\nconst timestamp = Date.now();\nconst newChatId = `chat_${body.projectName.replace(/\\s+/g, '_')}_${timestamp}`;\nconst newRagId = `rag_${body.projectName.replace(/\\s+/g, '_')}_${timestamp}`;\n\n// 3. Pass all the necessary data to the next node.\nreturn [{\n  json: {\n    projectName: body.projectName,\n    chat_session_id: newChatId,\n    rag_session_id: newRagId\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -512,
        -656
      ],
      "id": "5610c78f-9515-4c13-a0f6-a1aa5c3ac10a",
      "name": "Code2"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "fbb86eac-b086-4e9d-90a6-bce32d094885",
              "leftValue": "={{ $items.length }}",
              "rightValue": 0,
              "operator": {
                "type": "number",
                "operation": "gt"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -736,
        -768
      ],
      "id": "869ce699-6c58-4477-bf62-0909e2769862",
      "name": "If1",
      "executeOnce": false,
      "alwaysOutputData": false,
      "retryOnFail": false,
      "notesInFlow": false
    },
    {
      "parameters": {
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        -304,
        -864
      ],
      "id": "b19303bf-177b-43d7-b8a4-c2050ec42555",
      "name": "Respond to Webhook4"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ ({ chat_session_id: $('Code2').item.json.chat_session_id, rag_session_id: $('Code2').item.json.rag_session_id }) }}",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        -80,
        -656
      ],
      "id": "9b94d8da-617a-4721-9985-b6d12a9ba98a",
      "name": "Respond to Webhook5"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "a61a290c-d8e5-4c04-980a-4ebb415a21e4",
        "responseMode": "responseNode",
        "options": {
          "allowedOrigins": "*"
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -1184,
        -768
      ],
      "id": "d663c00b-147b-433e-8d0d-d398ee64d015",
      "name": "Webhook3",
      "webhookId": "a61a290c-d8e5-4c04-980a-4ebb415a21e4",
      "notesInFlow": false
    },
    {
      "parameters": {
        "content": "## System Documentation: Setup Workflow - Project Launcher\n\n\nWorkflow Name: Setup Workflow: Project Launcher for Documentation\nSource File: Setup_Workflow__Project_Launcher_for_Documentation.json\nProject Lead: Mike Holland\nSystem Architect: Gemini Pro\n1. Overall Goal\nThis workflow serves as the primary \"receptionist\" and setup engine for the Chat 8 UI. It is a dual-function workflow that automates both the creation of new, simple projects and the retrieval of the complete list of existing projects. Its core purpose is to manage the project_contexts table, ensuring each project has a unique set of identifiers before handing control back to the UI.\n2. Key Components & Architecture\nTrigger: A unique n8n Webhook (...a61a290c...) listens for POST requests from the Chat 8 UI's initial launcher screen.\nDatabase: It interacts exclusively with the project_contexts table in the Postgres database to create new project records and retrieve the full project list.\nLogic: The workflow uses an If node to act as a switch, directing the incoming request down one of two distinct paths based on an action property in the request body.\n3. Step-by-Step Data Flow\nThe workflow begins with a single entry point and immediately branches based on the user's intent.\nWebhook3: The workflow is triggered by a POST request from the UI. The payload contains a JSON body with an action key, which will be either create_project or list_projects.\nBranch A: \"List Existing Projects\"\nThis is the True path of the If1 node.\nGet Project List: A Postgres node executes a SELECT query to retrieve the project_name, chat_session_id, rag_session_id, and xata_id for all records in the project_contexts table.\nIf1: The workflow logic for this branch is slightly counter-intuitive. The If node checks if the result of the Get Project List query has more than 0 items. If True, it proceeds down this \"List Projects\" path.\nCode3: A Code node takes the list of projects from the database and transforms it to match the data contract expected by the UI, ensuring all necessary keys (project_name, session_id, project_id, rag_session_id) are present.\nRespond to Webhook4: The final Respond to Webhook node sends the formatted list of projects back to the UI, which then uses it to populate the project selection dropdown.\nBranch B: \"Create New Project\"\nThis is the False path of the If1 node.\nCode2: A Code node receives the projectName from the webhook. It generates a new, unique chat_session_id and rag_session_id based on the project name and the current timestamp.\nPostgres1: This Postgres node executes an INSERT query, using the data from the Code2 node to create a new record in the project_contexts table. It uses RETURNING xata_id to get the permanent project ID back from the database.\nRespond to Webhook5: The final Respond to Webhook node sends a success response back to the UI, providing it with the newly created chat_session_id and rag_session_id so it can initialize the new chat session.",
        "height": 80,
        "width": 752
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -64,
        -816
      ],
      "id": "7ef04a17-843e-4e50-8f76-5bf280d30332",
      "name": "Sticky Note2"
    },
    {
      "parameters": {
        "jsCode": "// Get all items from the previous node's output\nconst allInputItems = $input.all();\n\n// Use .map() to transform each item into the desired format\nconst allItems = allInputItems.map(item => {\n  return {\n    project_name: item.json.project_name,\n    session_id: item.json.chat_session_id,\n    project_id: item.json.xata_id,\n    rag_session_id: item.json.rag_session_id // <-- ADD THIS LINE\n  };\n});\n\n// Return the final, correctly structured object\nreturn {\n  json: {\n    projects: allItems\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -512,
        -864
      ],
      "id": "ecf73cca-e667-409c-9874-bfb0348dd509",
      "name": "Code3"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "d0b91f11-487b-441f-80a3-17edd5a703db",
        "responseMode": "responseNode",
        "options": {}
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -1264,
        -288
      ],
      "id": "fd27ac5c-6452-4a2f-a8c9-a4ad247f14f3",
      "name": "Webhook",
      "webhookId": "d0b91f11-487b-441f-80a3-17edd5a703db"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO project_contexts (project_name, session_id, status, lead_contact, description, notes, rag_session_id)\nVALUES ($1, $2, $3, $4, $5, $6, $7)\nRETURNING *;",
        "options": {
          "queryReplacement": "={{ $json.sql_params }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -592,
        -96
      ],
      "id": "e07c74a4-9f05-48ce-ba6e-82aa022e3bfe",
      "name": "Postgres",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        288,
        -96
      ],
      "id": "301f42bc-95e1-4c95-8d56-4690b426d28c",
      "name": "Respond to Webhook"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "{\n  \"error\": \"A project with this name already exists. Please choose a different name.\"\n}",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        -816,
        -288
      ],
      "id": "320e25f9-ce7c-41c2-9d41-458857ac2c42",
      "name": "Respond to Webhook1"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "afe88938-a540-43b8-9c58-c6f7e6903de1",
              "leftValue": "={{ $items(\"Check if Project Exists\").length }}",
              "rightValue": 0,
              "operator": {
                "type": "number",
                "operation": "gt"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -1040,
        -192
      ],
      "id": "6d2d4973-7003-46a4-9a6b-f3a9590290d3",
      "name": "If"
    },
    {
      "parameters": {
        "jsCode": "// Get the data from the previous node.\nconst data = $input.item.json;\n\n// --- FIX: Generate a NEW rag_session_id ---\n// We will create a unique ID for the RAG memory store.\nconst ragSessionId = 'rag_' + Math.random().toString(36).substring(2, 9);\n\n// Get the project name from the webhook's body property.\nconst projectName = data.body.projectName;\n\n// Generate a unique session ID for the project itself.\nconst sessionId = 'proj_' + Date.now() + '_' + Math.random().toString(36).substring(2, 9);\n\n// Set default values for the columns that actually exist.\nconst leadContact = \"\";\nconst status = \"New\";\nconst description = \"\";\nconst notes = \"\";\n\n// This is the chat session ID, which can be empty for a new project.\nconst chat_session_id = \"\"; \n\n// --- Assemble the final parameters for the SQL query ---\nconst sql_params = [\n  projectName,\n  sessionId,\n  status,\n  leadContact,\n  description,\n  notes,\n  ragSessionId // Use our newly generated ID\n];\n\n// Return the parameters for the next node.\nreturn {\n  json: {\n    sql_params: sql_params\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -816,
        -96
      ],
      "id": "580dc9b7-3208-4f34-8f0b-78ee961dde91",
      "name": "Code"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT project_name FROM project_contexts WHERE project_name = $1;",
        "options": {
          "queryReplacement": "=[{{ $json.body.projectName }}]"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -1040,
        -400
      ],
      "id": "fa661c50-406d-405d-a3b8-3c8223421207",
      "name": "Check if Project Exists",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{\n  ({\n    \"project_id\": $('Postgres').first().json.xata_id,\n    \"session_id\": $('Generate Thread ID').first().json.new_thread_id\n  })\n}}",
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        64,
        -96
      ],
      "id": "2e34df56-c719-4b52-a947-affe74dd73c6",
      "name": "Prepare Success Response",
      "notes": "test"
    },
    {
      "parameters": {
        "jsCode": "// Generate a unique session ID for the new thread\nconst new_thread_id = `session_${Math.random().toString(36).slice(2, 11)}`;\nreturn { new_thread_id: new_thread_id };"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -368,
        -96
      ],
      "id": "26c97860-d841-49e4-971d-0a8cf622e8ac",
      "name": "Generate Thread ID"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE project_contexts SET chat_session_id = $1 WHERE xata_id = $2;",
        "options": {
          "queryReplacement": "={{ $('Generate Thread ID').first().json.new_thread_id }}\n{{ $('Postgres').first().json.xata_id }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -160,
        -96
      ],
      "id": "5daee567-98ee-4c31-8d7f-6499d3b4daa0",
      "name": "Save Thread ID to Project",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "content": "## System Documentation: Create Project Workflow\n\n\nWorkflow Name: Create Project Workflow (as inferred from context)\nSource File: Main_Chat_Workflow_for_Documentation (1).json\nProject Lead: Mike Holland\nSystem Architect: Gemini Pro\n1. Overall Goal\nThis workflow serves as the dedicated backend process for creating a new, simple project. Its sole purpose is to receive a project name from the UI, ensure it doesn't already exist, create all the necessary records and IDs in the database, and return a success response to the UI so a new chat session can begin. It acts as a secure and robust entry point for Scenario 1.\n2. Key Components & Architecture\nTrigger: A unique n8n Webhook (...d0b91f11...) listens for POST requests from the Chat 8 UI.\nDatabase: It interacts exclusively with the project_contexts table in the Postgres database to check for duplicates and create new project records.\nLogic: The workflow follows a simple conditional path: if the project already exists, it fails gracefully; if not, it proceeds with creation.\n3. Step-by-Step Data Flow\nThe workflow executes in a clear, linear sequence:\nWebhook: The workflow is triggered when the user submits a new project name from the UI. The payload contains { \"projectName\": \"...\" }.\nCheck if Project Exists: A Postgres node immediately queries the project_contexts table to see if any record already has the provided projectName.\nIf (Project Exists?): An If node checks the result of the previous query.\nIf True (project exists): The workflow takes the True branch and immediately triggers the Respond to Webhook1 node, which sends a specific error message back to the UI, stopping the process.\nIf False (project is new): The workflow proceeds down the main False branch to create the project.\nCode (Prepare SQL Parameters): A Code node runs to prepare the data for the database INSERT command. It generates a unique rag_session_id and sessionId and assembles all the required values into an array (sql_params).\nPostgres (Create Project Record): This Postgres node executes the main INSERT query, using the sql_params from the previous step to create the new record in the project_contexts table. Crucially, it uses RETURNING * to output the full, newly created database row, including the permanent xata_id.\nGenerate Thread ID: A Code node generates a new, unique chat_session_id (formatted as session_...).\nSave Thread ID to Project: A Postgres node executes an UPDATE query. It takes the new_thread_id and the xata_id from the previous steps and updates the project record to include the new chat session ID.\nPrepare Success Response: A Set node takes the project_id (from the xata_id) and the session_id (from the new_thread_id) and formats them into a clean JSON object.\nRespond to Webhook: The final Respond to Webhook node sends this success object back to the UI, providing it with all the necessary IDs to initialize the new chat session.",
        "height": 80,
        "width": 600
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -480,
        -208
      ],
      "id": "132d5be5-7f38-4d02-ae08-c2b123b2936f",
      "name": "Sticky Note4"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/embeddings",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { \"model\": \"text-embedding-ada-002\", \"input\": $json.chunk_text_for_embedding } }}",
        "options": {
          "redirect": {},
          "response": {},
          "timeout": 120000
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -336,
        1008
      ],
      "id": "bf8f893e-d868-410a-83c5-ce7d3ac2c4bb",
      "name": "Create Embedding1",
      "executeOnce": false,
      "credentials": {
        "openAiApi": {
          "id": "4UBMhvVGj5H8Ehzq",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Code Node: Format Vector for Postgres\n// Outputting VECTOR STRING for n8n \"Execute Query\" operation\n\nconst inputJson = $input.item.json;\nlet vectorStringForDb = '[]'; // Default\nconst embeddingVectorArray = inputJson.embedding_vector_from_api;\n\nif (embeddingVectorArray && Array.isArray(embeddingVectorArray) && embeddingVectorArray.length > 0) {\n    // Create the string '[-0.01,0.02,...]'\n    vectorStringForDb = JSON.stringify(embeddingVectorArray).trim(); \n} else {\n    console.error(\"Format Vector for PG: embedding_vector_from_api was problematic. Using default '[]'. Chunk: \" + (inputJson.original_content_for_db || \"\").substring(0,30) );\n}\n\nconst outputData = {\n  session_id_for_insert: inputJson.session_id_for_insert,\n  original_content_for_db: inputJson.original_content_for_db,\n  role_for_final_insert: inputJson.role_for_db || inputJson['role-for-db'] || 'unknown',\n  embedding_vector_string_for_db: vectorStringForDb // Outputting the string\n};\n\nif (!outputData.session_id_for_insert || !outputData.original_content_for_db || !outputData.role_for_final_insert) {\n    console.error(\"Format Vector for PG: CRITICAL - Missing required fields. Data:\", outputData);\n    return null; \n}\n\nreturn outputData;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        336,
        928
      ],
      "id": "d7529c54-27a4-4913-af49-fb6b350a7ec9",
      "name": "Format Vector for Postgres1"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO rag_store (project_id, session_id, original_content, role, embedding) VALUES ($1, $2, $3, $4, $5);",
        "options": {
          "queryBatching": "independently",
          "queryReplacement": "={{ [ $item(0).$node[\"Create Project Entry1\"].json[\"project_id\"], $item(0).$node[\"Create Project Entry1\"].json[\"rag_session_id\"], $json.original_content_for_db, $json.role_for_final_insert, $json.embedding_vector_string_for_db ] }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        560,
        928
      ],
      "id": "8de7c71d-b0d3-4167-a677-267fab5e39eb",
      "name": "Insert Embedding to Xata1",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "df1df400-e07f-4834-a324-6f644c650e0d",
              "name": "session_id_for_insert",
              "value": "={{ $json.session_id_for_db }}",
              "type": "string"
            },
            {
              "id": "98488e14-449e-43ab-8f11-7bbaf90fb2db",
              "name": "original_content_for_db",
              "value": "={{ $json.chunk_text_for_embedding }}",
              "type": "string"
            },
            {
              "id": "605e03fe-51ff-434d-89a1-1b8c71126cc7",
              "name": "role-for-db",
              "value": "={{ ($json.original_role_for_db === 'model' || $json.original_role_for_db === 'assistant') ? 'assistant' : 'user' }}",
              "type": "string"
            },
            {
              "id": "21b390bb-4fa5-461d-a3e6-613ff3f5e823",
              "name": "embedding_vector_from_api",
              "value": "={{ $json.data[0].embedding }}",
              "type": "array"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        112,
        928
      ],
      "id": "989c1748-01c4-420d-b5ad-6ab63b62240a",
      "name": "Prepare for DB1"
    },
    {
      "parameters": {
        "url": "={{ $('Webhook2').item.json.body.jsonUrl }}",
        "options": {
          "redirect": {},
          "response": {
            "response": {
              "fullResponse": false,
              "neverError": false,
              "responseFormat": "text",
              "outputPropertyName": "data"
            }
          },
          "timeout": 60000
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -992,
        928
      ],
      "id": "be0d3c3c-f081-4924-8b88-fd4652677dbc",
      "name": "HTTP Request GCS Fetch1"
    },
    {
      "parameters": {
        "jsCode": "// --- Intelligent Chunking Script v3 (with Debugging) ---\n\n// This line now gets the session_id from the node that created the project in the database.\nconst BATCH_SESSION_ID = $('Webhook2').item.json.body.projectName;\nconst MAX_CHUNK_SIZE_CHARS = 6000; // Safety limit (~1500 tokens)\nconst documentText = $json.data;\nconst finalItems = [];\n\n// Split the document by our manual break first.\nconst primaryChunks = documentText.split('---CHUNK_BREAK---');\n\nfor (const primaryChunk of primaryChunks) {\n    let currentChunk = primaryChunk.trim();\n    if (!currentChunk) continue;\n\n    // If a chunk is already small enough, process it.\n    if (currentChunk.length <= MAX_CHUNK_SIZE_CHARS) {\n        // --- DEBUGGING ---\n        console.log(`[OK] Chunk size: ${currentChunk.length}`);\n        finalItems.push({\n            json: {\n                chunk_text_for_embedding: currentChunk,\n                session_id_for_db: BATCH_SESSION_ID\n            }\n        });\n        continue;\n    }\n\n    // If a chunk is TOO LARGE, we must split it further.\n    // --- DEBUGGING ---\n    console.log(`[LARGE CHUNK DETECTED] Splitting chunk of size: ${currentChunk.length}`);\n    let tempChunk = \"\";\n    const sentences = currentChunk.split(/(?<=[.?!])\\s+/); // Split by sentences\n\n    for (const sentence of sentences) {\n        if ((tempChunk.length + sentence.length + 1) > MAX_CHUNK_SIZE_CHARS) {\n            // --- DEBUGGING ---\n            console.log(`[SUB-CHUNK CREATED] Size: ${tempChunk.length}`);\n            finalItems.push({\n                json: {\n                    chunk_text_for_embedding: tempChunk,\n                    session_id_for_db: BATCH_SESSION_ID\n                }\n            });\n            tempChunk = sentence;\n        } else {\n            tempChunk += (tempChunk ? \" \" : \"\") + sentence;\n        }\n    }\n    // Add the last remaining part\n    if (tempChunk) {\n        // --- DEBUGGING ---\n        console.log(`[FINAL SUB-CHUNK] Size: ${tempChunk.length}`);\n        finalItems.push({\n            json: {\n                chunk_text_for_embedding: tempChunk,\n                session_id_for_db: BATCH_SESSION_ID\n            }\n        });\n    }\n}\n\nreturn finalItems;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -768,
        928
      ],
      "id": "27ff3bee-8dc0-426b-a87a-bdadeb366a5e",
      "name": "Chunk Code1"
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineByPosition",
        "options": {}
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        -112,
        928
      ],
      "id": "b8bdb335-9b5e-4fd6-8242-1e64863d8fa0",
      "name": "Merge Embedding with Chunk Data1"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "95b4011b-5ede-4775-a952-bd6963670ce9",
              "leftValue": "={{ $json.chunk_text_for_embedding }}",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "notEmpty",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2.2,
      "position": [
        -560,
        928
      ],
      "id": "7285a2b0-4a4c-4e4d-bc75-4d604d0eef30",
      "name": "Filter1"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "WITH generated_ids AS (\n    SELECT\n        (EXTRACT(EPOCH FROM NOW())::BIGINT)::text AS project_id,\n        'chat_' || (EXTRACT(EPOCH FROM NOW())::BIGINT) AS chat_session_id,\n        'rag_' || REPLACE('{{$json.body.projectName}}', ' ', '_') || '_' || (EXTRACT(EPOCH FROM NOW())::BIGINT) AS rag_session_id\n),\ninserted_project AS (\n    INSERT INTO project_contexts (\n        project_name,\n        session_id,\n        chat_session_id,\n        rag_session_id,\n        status,\n        description,\n        notes,\n        lead_contact\n    )\n    SELECT\n        '{{$json.body.projectName}}',\n        g.chat_session_id,\n        g.chat_session_id,\n        g.rag_session_id,\n        'Active',\n        'Project created from Indexed URL',\n        'Source URL: {{$json.body.jsonUrl}}',\n        'Project created from URL'\n    FROM generated_ids g\n)\nSELECT project_id, chat_session_id, rag_session_id FROM generated_ids;",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -1216,
        928
      ],
      "id": "2eecde3c-74aa-4b42-a933-01ff6d8bdbdc",
      "name": "Create Project Entry1",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{\n  ({\n    status: 'success',\n    project_id: $('Create Project Entry1').first().json.project_id,\n    session_id: $('Create Project Entry1').first().json.session_id,\n    rag_session_id: $('Create Project Entry1').first().json.rag_session_id\n  })\n}}",
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        784,
        928
      ],
      "id": "36abbeed-6d72-4af8-bea1-23447b31ab23",
      "name": "Prepare Final Response1",
      "executeOnce": true
    },
    {
      "parameters": {
        "content": "## System Documentation: RAG Indexing Workflow\n\n**IMPLEMENTATION NOTES - TIMEOUT FIX APPLIED**\nThis workflow has been updated with timeout configurations to resolve HTTP request failures in VM Docker environments.\n- Create Embedding1 node: 120 second timeout added\n- HTTP Request GCS Fetch1 node: 60 second timeout added\n- Workflow execution timeout: 1 hour limit\n- Error handling: saveManualExecutions enabled for debugging\n\nRECOMMENDED RETRY LOGIC (for future enhancement):\n- Add Code node after each HTTP Request for exponential backoff retry\n- Implement 3 retry attempts with 5-second delays\n- Log timeout errors for monitoring and alerting\n\n\nWorkflow Name: RAG Indexing Workflow for Documentation\nSource File: RAG_Indexing_Workflow_for_Documentation.json\nProject Lead: Mike Holland\nSystem Architect: Gemini Pro\n1. Overall Goal\nThis workflow serves as the dedicated data pipeline for bootstrapping a new project with an existing knowledge base. Its purpose is to receive a URL to a JSON file and a new project name from the UI, create the necessary project records in the database, and then systematically process the contents of the JSON file. It chunks the text, generates vector embeddings for each chunk, and saves them to the long-term memory store (rag_store), permanently associating them with the newly created project.\n2. Key Components & Architecture\nTrigger: A unique n8n Webhook (...303639ed...) listens for POST requests from the Chat 8 UI's \"Create from URL\" feature.\nDatabase: It interacts with both the project_contexts table (to create the new project) and the rag_store table (to save the indexed memories).\nExternal Services: It makes calls to Google Cloud Storage (GCS) to fetch the source JSON and to the OpenAI API to generate embeddings.\nLogic: The workflow is a sequential pipeline: create the project, fetch the data, chunk it, create embeddings, and save everything to the database.\n3. Step-by-Step Data Flow\nThe workflow executes in a clear, linear sequence:\nWebhook2: The workflow is triggered when the user submits a JSON URL and a new project name from the UI. The payload contains { \"projectName\": \"...\", \"jsonUrl\": \"...\" }.\nCreate Project Entry1: A Postgres node immediately executes a complex SQL query. This single query is responsible for:\nGenerating a new, unique project_id (numeric), chat_session_id, and rag_session_id.\nInserting a new record into the project_contexts table with these new IDs and the provided project name.\nReturning the newly generated IDs for use in subsequent steps.\nHTTP Request GCS Fetch1: An HTTPRequest node takes the jsonUrl from the initial webhook and fetches the raw text content of the JSON file from Google Cloud Storage. **TIMEOUT: 60 seconds**\nChunk Code1: A sophisticated Code node takes the raw text from the file. It intelligently splits the text into smaller, manageable chunks suitable for embedding, ensuring no chunk exceeds a maximum character limit. It outputs a list of items, each containing a text chunk.\nFilter1: A simple Filter node ensures that no empty chunks proceed, preventing wasted API calls.\nCreate Embedding1: (Runs for each chunk) An HTTPRequest node takes each text chunk and sends it to the OpenAI embeddings API (text-embedding-ada-002) to generate a vector embedding. **TIMEOUT: 120 seconds**\nMerge Embedding with Chunk Data1: A Merge node combines the original chunk data with the newly generated embedding vector from the previous step.\nPrepare for DB1: A Set node takes the merged data and restructures it, creating clean fields (session_id_for_insert, original_content_for_db, etc.) ready for the next step.\nFormat Vector for Postgres1: A Code node takes the embedding vector (which is a JavaScript array) and converts it into the specific string format (e.g., \"[0.1, 0.2, ...]\") required by the pgvector database type.\nInsert Embedding to Xata1: (Runs for each chunk) This is the final, critical database operation. A Postgres node takes the fully prepared data for each chunk and executes an INSERT query to save it to the rag_store table. It correctly links each memory to the project_id and rag_session_id that were generated in Step 2.\nPrepare Final Response1: After all chunks have been successfully inserted, a Set node runs once to format a clean success message.\nRespond to Webhook3: The final Respond to Webhook node sends the success object (containing the new project_id, session_id, and rag_session_id) back to the UI, signaling that the project has been created and indexed, and a new chat session can begin.",
        "height": 80,
        "width": 620
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -48,
        1136
      ],
      "id": "d9e5e2a8-2f45-4833-9017-3110a8e7f0b9",
      "name": "Sticky Note1"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={\n  \"project_id\": \"{{$item(0).$node[\"Create Project Entry1\"].json[\"project_id\"]}}\",\n  \"session_id\": \"{{$item(0).$node[\"Create Project Entry1\"].json[\"chat_session_id\"]}}\",\n  \"rag_session_id\": \"{{$item(0).$node[\"Create Project Entry1\"].json[\"rag_session_id\"]}}\"\n}",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        1008,
        928
      ],
      "id": "746edd63-31f4-4179-bd14-f8d0bb0593c8",
      "name": "Respond to Webhook3"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "303639ed-a3e2-4eae-b406-16e1c6200a81",
        "responseMode": "responseNode",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "http://127.0.0.1:5500"
              },
              {
                "name": "Access-Control-Allow-Methods",
                "value": "OPTIONS and POST"
              },
              {
                "name": "Access-Control-Allow-Headers",
                "value": "Content-Type"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -1440,
        928
      ],
      "id": "42d6cf6a-edb1-410f-953f-66a3e2b0f46c",
      "name": "Webhook2",
      "webhookId": "303639ed-a3e2-4eae-b406-16e1c6200a81"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://n8n.srv997771.hstgr.cloud/webhook/ai-router",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { \"selected_model\": $json.model, \"messages\": $json.messages } }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        1152,
        1904
      ],
      "id": "a5708a2a-4eb8-451d-9a3e-de8a9b23c3bb",
      "name": "HTTP Request to AI Router"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/*MIKE_MARKER:CODE1_ES5_V3_AI_ROUTER*/ \nvar r=$input.item&&$input.item.json?$input.item.json:{}; \nvar replyText='Error: Could not extract reply.'; \nvar toast; \n\n// Handle AI Router response format\nif(r && r.success && typeof r.content === 'string') {\n  var raw = String(r.content); \n  var cleaned = raw.replace(/[^\\x20-\\x7E\\n\\r\\t]+/g,''); \n  var maxLen = 15000; \n  if(cleaned.length > maxLen) { \n    cleaned = cleaned.substring(0, maxLen) + '... (truncated)'; \n  } \n  replyText = cleaned;\n  \n  // Handle auto-commit status messages\n  try { \n    var an = $node['Auto Commit via Webhook'].first(); \n    if(an && an.json) { \n      var statusLine = an.json.status_line; \n      var toastLine = an.json.toast; \n      if(statusLine) { \n        replyText += '\\n\\n' + statusLine; \n      } \n      if(toastLine) { \n        toast = toastLine; \n      } \n    } \n  } catch(e) {}\n} else if(r && !r.success) {\n  replyText = 'AI Router Error: ' + (r.error || 'Unknown error occurred');\n}\n\n// Get model info from Build OpenAI Payload1\nvar selected_model; \nvar usedVision = false; \ntry { \n  var pn = $node['Build OpenAI Payload1'].first(); \n  if(pn && pn.json) { \n    selected_model = pn.json.model; \n    usedVision = !!pn.json.usedVision; \n  } \n  if(!usedVision) { \n    var fci = $node['Format Current Input'].first(); \n    if(fci && fci.json && Object.prototype.toString.call(fci.json.content) === '[object Array]') { \n      var arr = fci.json.content; \n      for(var a = 0; a < arr.length; a++) { \n        var pp = arr[a]; \n        if(pp && pp.type === 'image_url') { \n          usedVision = true; \n          break; \n        } \n      } \n    } \n  } \n} catch(e) {} \n\nvar out = { reply: replyText }; \nif(selected_model) out.selected_model = selected_model; \nout.usedVision = usedVision; \nif(toast) out.toast = toast; \nif(r && r.provider) out.ai_provider = r.provider; \nreturn out;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1376,
        1904
      ],
      "id": "fd1fd44d-c2c6-4753-bee8-dd67d656d869",
      "name": "Code1"
    },
    {
      "parameters": {
        "jsCode": "// --- Prepare Consolidated RAG Context v1.0 ---\n// This script takes the single block of consolidated RAG text\n// and formats it correctly for the final prompt.\n\n// Get the combined context from the previous node's output.\nconst combinedContext = $input.item.json.combined_rag_context;\n\n// Create a single \"context\" object that the Merge node can use.\n// We will assign it the role of \"system\" to indicate it's background info.\nconst formattedRagContext = {\n  role: 'system',\n  content: `--- CONTEXT FROM KNOWLEDGE BASE ---\\n${combinedContext}`\n};\n\n// Return a single item containing this formatted object.\nreturn [{\n  json: formattedRagContext\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        496,
        1824
      ],
      "id": "340c8442-9188-4ff3-9807-ca65c6fcd02d",
      "name": "Format History for AI",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/embeddings",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{\n  {\n    \"model\": \"text-embedding-3-small\",\n    \"input\": $('Get Session ID & Input').item.json.chatInput\n  }\n}}",
        "options": {
          "response": {
            "response": {
              "responseFormat": "json"
            }
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -608,
        1824
      ],
      "id": "0e58b4e9-d4e4-430c-b9d9-6094d7f0642c",
      "name": "Create Query Embedding",
      "credentials": {
        "openAiApi": {
          "id": "MjVoArzZMRHgjBwn",
          "name": "OpenAi (Embeddings)"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// This node receives two inputs.\n// Input 0: From \"Create Query Embedding\"\n// Input 1: From the initial Webhook trigger\n\n// Get the embedding from the embedding creation node (input 0)\nconst embeddingData = $input.all(0)[0].json;\nconst embedding = embeddingData.data[0].embedding;\n\n// Get the data directly from the initial Webhook trigger node by its name\nconst triggerData = $('Webhook1').first().json;\n\n// Get the RAG session ID from the webhook's parsed JSON body\nconst ragSessionId = triggerData.body.rag_session_id;\n\n// Format the embedding vector into the string format for pgvector.\nconst query_embedding_for_pg = '[' + embedding.join(',') + ']';\n\n// Create the final output object.\n// It's crucial that this object has a 'json' property containing our data.\nreturn {\n  json: {\n    query_embedding_for_pg: query_embedding_for_pg,\n    current_session_id_for_search: ragSessionId\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -384,
        1824
      ],
      "id": "01164a87-9102-4b8e-80a2-17f3b06c2d32",
      "name": "Format Data for Vector Search"
    },
    {
      "parameters": {
        "jsCode": "const body = $input.item.json.body;\n\nif (!body.chat_session_id || typeof body.chat_session_id !== 'string') {\n  throw new Error(\"The 'chat_session_id' field is missing or invalid in the webhook body.\");\n}\nif (!body.chatInput && !body.imageUrl && !body.imageData) {\n  throw new Error(\"The 'chatInput' field, 'imageUrl', or 'imageData' must be provided in the webhook body.\");\n}\n\n// Pass ALL required IDs and image info downstream. Prefer URL.\nreturn [{\n  json: {\n    chatInput: body.chatInput,\n    imageUrl: body.imageUrl || null,\n    imageData: body.imageData || null,\n    session_Id: body.chat_session_id,\n    rag_session_Id: body.rag_session_id,\n    project_Id: body.project_id\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1488,
        1648
      ],
      "id": "3b79857c-d9b4-485a-a7c2-626b5b6bb255",
      "name": "Get Session ID & Input"
    },
    {
      "parameters": {
        "content": "## System Documentation: Main Chat Workflow\n\n\nWorkflow Name: Main Chat Workflow for Documentation\nVersion: As of file provided\nProject Lead: Mike Holland\nSystem Architect: Gemini Pro\n1. Overall Goal\nThis n8n workflow serves as the central orchestration layer for the Chat 8 user interface. Its primary purpose is to receive user input, intelligently enrich it with multiple forms of contextlong-term memory from a RAG store and short-term conversational historyand generate a context-aware response from a large language model. It also includes a new feature for automatically committing \"remember\" intents to memory.\n2. Key Components & Architecture\nThe system is comprised of several key components that work in concert:\nFrontend: The Chat 8 UI (Chat_8_V7.html) captures user input and displays the final response.\nBackend Orchestration: This n8n workflow manages the entire data flow and logic.\nLong-Term Memory: A Postgres database containing the rag_store for vectorized knowledge.\nShort-Term Memory: The same Postgres database, using the conversation_history table for recent conversational turns.\nThe workflow's core architecture is a multi-branch parallel processing system. After initial input, it splits into distinct paths to gather different types of context, which are then merged before the final AI call.\n3. Step-by-Step Data Flow\nThe workflow executes in several distinct phases:\nPhase 1: Ingestion and Preparation\nWebhook1: The workflow is triggered by a POST request from the UI. It is configured to handle pre-flight OPTIONS requests for CORS compatibility.\nGet Session ID & Input: A Code node parses the incoming request body. It validates that chat_session_id and chatInput are present and transforms all key IDs to a consistent camelCase format (session_Id, rag_session_Id, project_Id) for use within the workflow.\nSave User Message to History: A Postgres node immediately saves the user's message to the conversation_history table, ensuring a complete and persistent log of the interaction.\nPreserve Current Inputs: A critical Set node creates a stable, preserved copy of the key inputs (session_Id, chatInput, rag_session_Id, etc.). This node acts as a central hub, providing a reliable data source for all subsequent parallel branches.\nPhase 2: Auto-Commit Branch (Side Process)\nRunning in parallel to the main chat logic.\nConfig: Auto-Commit Enabled: A Set node acts as a feature flag, enabling the auto-commit functionality.\nIf Remember Intent: An If node checks if the user's chatInput starts with the word \"remember\".\nAuto Commit via Webhook: If the intent is to remember, an HTTPRequest node triggers the separate \"Commit to Memory\" workflow, passing the necessary IDs to save the new fact in the background.\nPhase 3: Main Logic Branching\nIf Rag is Active: This is the primary traffic controller. It checks if a rag_session_Id was provided.\nIf True: The workflow proceeds down the full RAG path to retrieve long-term memory.\nIf False: The workflow bypasses the RAG steps and proceeds directly to retrieve only the short-term conversational history.\nPhase 4: The RAG Path (Dual-Retrieval)\nThis path executes if RAG is active.\nCreate Query Embedding: An HTTPRequest node takes the user's chatInput and calls the OpenAI API to convert it into a vector embedding.\nFormat Data for Vector Search: A Code node prepares the data for the database search, formatting the embedding vector into the required string format for pgvector.\nParallel Retrieval: The workflow splits again to perform two simultaneous database lookups:\nRetrieve Committed Memory: A Postgres node searches the rag_store for memories from the live conversation.\nRetrieve RAG Chunks: A Postgres node searches the rag_store for memories from the initial bootstrapped knowledge base.\nMerge1: A Merge node combines the results from both retrieval steps into a single list.\nRAG Context consolidator: A Code node takes the merged list, removes any duplicate memories, and formats the unique results into a single, clean block of text.\nFormat History for AI: A Code node takes the consolidated text and wraps it in a standard { role: 'system', content: '...' } object, ready for the final prompt.\nPhase 5: Final Prompt Assembly & AI Call\nGet Recent history: (Runs in parallel to the RAG path) A Postgres node queries the conversation_history table for the last 6 turns of the conversation.\nFormat Recent History: A Code node formats these turns into the standard OpenAI message format.\nFormat Current Input: A Code node formats the user's current message into the standard format.\nMerge: This is the final assembly point. A Merge node combines the three streams of context:\nInput 1: The RAG context (from Format History for AI).\nInput 2: The recent conversational history (from Format Recent History).\nInput 3: The user's current message (from Format Current Input).\nBuild OpenAI Payload1: A sophisticated Code node takes the fully merged data. It intelligently selects the correct system prompt (persona), assembles the final message array in the correct order (system prompt, RAG context, history, user message), and builds the complete JSON payload for the AI.\nHTTP Request1: Sends the final payload to the OpenAI Chat Completions API.\nPhase 6: Response and Finalization\nCode1: A Code node parses the response from OpenAI, extracts the AI's reply, and performs cleaning and truncation. It also checks for any status messages from the background Auto-Commit process and appends them to the reply.\nPostgres2 (Save AI Reply): Saves the AI's generated response back to the conversation_history table.\nRespond to Webhook2: Sends the final, clean reply back to the Chat 8 UI, completing the cycle.",
        "height": 80,
        "width": 548
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -880,
        1488
      ],
      "id": "9d8364d9-efc6-4732-b4dd-93b7aabeb8e9",
      "name": "Sticky Note"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT role, content\nFROM conversation_history\nWHERE session_id = $1\nORDER BY created_at DESC\nLIMIT 6; -- Fetch more (e.g., 3 user, 3 AI) to ensure we get a few full turns",
        "options": {
          "queryReplacement": "={{ [$json.session_Id] }}"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        272,
        2112
      ],
      "id": "090ba34f-7711-47c6-8924-4208db01adf5",
      "name": "Get Recent history",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Code Node: Format Recent History\n// Input: items from \"Get Recent History\" node (array of {role, content} objects, newest first)\n// Output: A single item containing a 'recent_history' array, formatted for OpenAI (oldest first)\n\nconst inputItems = $input.all(); // Get all input items (each is a row from DB)\nlet formattedHistory = [];\n\n// Check if there are any input items\nif (inputItems.length > 0) {\n    // Extract .json from each item to get the actual data rows\n    const historyRecords = inputItems.map(item => item.json); \n\n    if (historyRecords && historyRecords.length > 0) {\n      // The history from DB is newest first (ORDER BY created_at DESC), \n      // so reverse it to get oldest first for the OpenAI prompt\n      formattedHistory = historyRecords.reverse().map(record => {\n        let roleToUse = 'unknown';\n        if (typeof record.role === 'string') {\n            const dbRole = record.role.toLowerCase();\n            if (dbRole === 'user') {\n                roleToUse = 'user';\n            } else if (dbRole === 'assistant' || dbRole === 'ai' || dbRole === 'model') {\n                roleToUse = 'assistant';\n            } else {\n                console.warn(`Format Recent History: Unknown role '${dbRole}' found, mapping to 'user'. Content: ${(record.content || \"\").substring(0,50)}`);\n                roleToUse = 'user'; // Fallback for unknown roles\n            }\n        } else {\n             console.warn(`Format Recent History: Missing or invalid role for record. Defaulting to 'user'. Record:`, record);\n             roleToUse = 'user';\n        }\n        const content = (typeof record.content === 'string') ? record.content : '';\n        \n        return {\n          role: roleToUse, \n          content: content\n        };\n      });\n    }\n}\n\n// Output a single item, with the formatted history array in a property\nreturn formattedHistory.map(item => ({ json: item }));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        496,
        2112
      ],
      "id": "34114b19-ad8f-4340-a3f2-e34b01af6e96",
      "name": "Format Recent History",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "schema": {
          "__rl": true,
          "mode": "list",
          "value": "public"
        },
        "table": {
          "__rl": true,
          "value": "conversation_history",
          "mode": "list",
          "cachedResultName": "conversation_history"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "session_id": "={{ $json.session_Id }}",
            "role": "user",
            "content": "={{ $json.chatInput }}"
          },
          "matchingColumns": [
            "id"
          ],
          "schema": [
            {
              "id": "id",
              "displayName": "id",
              "required": false,
              "defaultMatch": true,
              "display": true,
              "type": "number",
              "canBeUsedToMatch": true,
              "removed": true
            },
            {
              "id": "session_id",
              "displayName": "session_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "role",
              "displayName": "role",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "content",
              "displayName": "content",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "created_at",
              "displayName": "created_at",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "dateTime",
              "canBeUsedToMatch": true,
              "removed": true
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -1264,
        1648
      ],
      "id": "7913a635-48b5-43dd-bab1-49d389aeeadc",
      "name": "Save User Message to History",
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "ecc57f20-7f83-4ed7-bc40-a63bf0b318b7",
              "name": "session_Id",
              "value": "={{ $('Get Session ID & Input').item.json.session_Id }}",
              "type": "string"
            },
            {
              "id": "1454ec7a-35ff-4bf1-aaf6-0bd559c6ddca",
              "name": "chatInput",
              "value": "={{ $('Get Session ID & Input').item.json.chatInput }}",
              "type": "string"
            },
            {
              "id": "9f0a5ab1-1111-4a2e-aaaa-1234567890ab",
              "name": "imageUrl",
              "value": "={{ $('Get Session ID & Input').item.json.imageUrl }}",
              "type": "string"
            },
            {
              "id": "e45f0fda-84cf-43e7-94ff-41ed896ce7e8",
              "name": "imageData",
              "value": "={{ $('Get Session ID & Input').item.json.imageData }}",
              "type": "object"
            },
            {
              "id": "09c1b52a-2cf0-4101-8a06-8ae66e1d8ea9",
              "name": "rag_session_Id",
              "value": "={{ $('Get Session ID & Input').item.json.rag_session_Id }}",
              "type": "string"
            },
            {
              "id": "5c53893f-4d94-4559-bef2-dc3123f2ae94",
              "name": "project_Id",
              "value": "={{ $('Get Session ID & Input').item.json.project_Id }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -1040,
        1648
      ],
      "id": "679891c0-d415-4acb-a2e9-982360d3666d",
      "name": "Preserve Current Inputs"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "b6a2f6aa-4b9d-4a9e-b0e3-6e1e57e7f4f1",
              "name": "autoCommitEnabled",
              "value": "true",
              "type": "boolean"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -816,
        1632
      ],
      "id": "d47ea5c4-ebdd-4fab-8fbc-f99a57fb0a3c",
      "name": "Config: Auto-Commit Enabled"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": false,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "9b2a5e2c-30f2-4e2e-9e0d-7fbf5c7b8bd1",
              "leftValue": "={{ $json.chatInput }}",
              "rightValue": "^\\s*remember\\b",
              "operator": {
                "type": "string",
                "operation": "regex",
                "singleValue": true
              }
            },
            {
              "id": "f4d0a1b2-5e6f-4a7b-8c9d-1e2f3a4b5c6d",
              "leftValue": "={{ $('Config: Auto-Commit Enabled').first().json.autoCommitEnabled }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -608,
        1632
      ],
      "id": "91023feb-cdc3-47bc-a15c-bc5b91387d62",
      "name": "If Remember Intent"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://n8n.srv997771.hstgr.cloud/webhook/6c1ce608-2f7a-457b-9afc-f0be5ef4bd4c",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { chat_session_id: $('Preserve Current Inputs').first().json.session_Id, rag_session_id: $('Preserve Current Inputs').first().json.rag_session_Id } }}",
        "options": {
          "response": {
            "response": {
              "responseFormat": "json"
            }
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -384,
        1632
      ],
      "id": "dad03daa-4b55-4018-b090-11e481b67e17",
      "name": "Auto Commit via Webhook"
    },
    {
      "parameters": {
        "schema": {
          "__rl": true,
          "mode": "list",
          "value": "public"
        },
        "table": {
          "__rl": true,
          "value": "conversation_history",
          "mode": "list",
          "cachedResultName": "conversation_history"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "session_id": "={{ $('Preserve Current Inputs').first().json.session_Id }}",
            "role": "'assistant'",
            "content": "={{ $json.reply }}"
          },
          "matchingColumns": [
            "id"
          ],
          "schema": [
            {
              "id": "id",
              "displayName": "id",
              "required": false,
              "defaultMatch": true,
              "display": true,
              "type": "number",
              "canBeUsedToMatch": true,
              "removed": true
            },
            {
              "id": "session_id",
              "displayName": "session_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "role",
              "displayName": "role",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "content",
              "displayName": "content",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "created_at",
              "displayName": "created_at",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "dateTime",
              "canBeUsedToMatch": true,
              "removed": true
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1600,
        1712
      ],
      "id": "99d9bb51-20eb-4761-b4d1-fc6f9c04ff1d",
      "name": "Postgres2 (Save AI Reply)",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Get all items from all inputs\nconst allItems = $input.all();\n\n// Gracefully handle cases where one or both searches return no results.\nif (allItems.length === 0) {\n  return [{ json: { combined_rag_context: \"No relevant context was found in the knowledge base.\" } }];\n}\n\n// Use a Set to automatically handle duplicates\nconst uniqueContent = new Set();\nallItems.forEach(item => {\n  if (item.json.original_content) {\n    uniqueContent.add(item.json.original_content);\n  }\n});\n\n// Join the unique snippets together\nconst combinedText = Array.from(uniqueContent).join('\\n---\\n');\n\nreturn [{\n  json: {\n    combined_rag_context: combinedText\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        272,
        1824
      ],
      "id": "11335d1f-fadb-461d-81d2-0f3c96a79600",
      "name": "RAG Context consolidator",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "numberInputs": 3
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        720,
        1904
      ],
      "id": "d77e4612-2310-4c47-8c70-1f93ef577d75",
      "name": "Merge",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "jsCode": "/*MIKE_MARKER:FCI_v3_SIMPLIFIED*/\n// Get user message\nvar userMessage = '';\nif ($input.item && $input.item.json && $input.item.json.chatInput) {\n  userMessage = String($input.item.json.chatInput);\n}\n\n// Get image data - check current input first\nvar imageUrl = null;\nvar imageData = null;\n\nif ($input.item && $input.item.json) {\n  imageUrl = $input.item.json.imageUrl;\n  imageData = $input.item.json.imageData;\n}\n\nconsole.log('FCI v3 - userMessage:', userMessage);\nconsole.log('FCI v3 - imageUrl from input:', imageUrl);\nconsole.log('FCI v3 - imageData from input:', imageData);\n\n// Check if we have a valid image URL\nvar hasImageUrl = !!(imageUrl && typeof imageUrl === 'string' && imageUrl.length > 0 && imageUrl !== 'null');\n\n// Check if we have inline image data\nvar hasImageData = !!(imageData && imageData.dataUri);\n\nvar hasAnyImage = hasImageUrl || hasImageData;\nvar hasText = !!(userMessage && userMessage.length > 0);\n\nconsole.log('FCI v3 - hasImageUrl:', hasImageUrl);\nconsole.log('FCI v3 - hasImageData:', hasImageData);\nconsole.log('FCI v3 - hasAnyImage:', hasAnyImage);\nconsole.log('FCI v3 - hasText:', hasText);\n\nif (!hasText && !hasAnyImage) {\n  return [{ json: { role: 'user', content: '' } }];\n}\n\nvar content;\nif (hasAnyImage) {\n  var finalUrl = hasImageUrl ? imageUrl : imageData.dataUri;\n  content = [\n    { type: 'text', text: hasText ? userMessage : '' },\n    { type: 'image_url', image_url: { url: finalUrl, detail: 'auto' } }\n  ];\n  console.log('FCI v3 - Created VISION content:', JSON.stringify(content));\n} else {\n  content = userMessage;\n  console.log('FCI v3 - Created TEXT-ONLY content');\n}\n\nreturn [{ json: { role: 'user', content: content } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        496,
        1440
      ],
      "id": "2d7759c1-b162-4282-a297-6ac8ad87f1d8",
      "name": "Format Current Input",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "jsCode": "// Build OpenAI Payload1 - Enhanced with Model Selection\n// Input: Merged items from Format History for AI, Format Recent History, and Format Current Input\n\nvar DEFAULT_TEXT_MODEL = 'claude-4-sonnet';\nvar VISION_MODEL = 'gpt-4o';\n\n// --- Model Selection Enhancement ---\nvar frontendModel = '';\nvar modelType = 'openai';\nvar routeTo = 'openai';\n\ntry {\n  // Try the method that works in other nodes\n  var wb = null;\n  \n  try {\n    var triggerData = $('Webhook1').first().json;\n    wb = triggerData ? triggerData.body : null;\n    console.log(' Method SUCCESS - $(Webhook1).first().json.body:', wb ? 'found' : 'not found');\n    if (wb) {\n      console.log(' Webhook body:', JSON.stringify(wb, null, 2));\n    }\n  } catch (e) {\n    console.log(' Method failed:', e.message);\n  }\n  \n  console.log(' selected_model value:', wb ? wb.selected_model : 'no webhook data');\n  console.log(' selected_model type:', wb && wb.selected_model ? typeof wb.selected_model : 'undefined');\n  \n  if (wb && wb.selected_model && typeof wb.selected_model === 'string' && wb.selected_model.trim()) {\n    frontendModel = wb.selected_model.trim();\n    console.log(' Frontend model set to:', frontendModel);\n    \n    // Determine model type and routing\n    if (frontendModel.toLowerCase().includes('claude')) {\n      modelType = 'claude';\n      routeTo = 'claude';\n      console.log(' Routing to Claude');\n    } else if (frontendModel.toLowerCase().includes('gpt')) {\n      modelType = 'openai';\n      routeTo = 'openai';\n      console.log(' Routing to OpenAI');\n    }\n  } else {\n    console.log(' selected_model condition failed:', {\n      hasWb: !!wb,\n      hasSelectedModel: wb ? !!wb.selected_model : false,\n      selectedModelValue: wb ? wb.selected_model : 'no wb',\n      selectedModelType: wb && wb.selected_model ? typeof wb.selected_model : 'no type',\n      trimTest: wb && wb.selected_model ? wb.selected_model.trim() : 'no trim'\n    });\n  }\n  \n  // Legacy vision model override\n  if (wb && wb.vision_model) {\n    VISION_MODEL = wb.vision_model.trim();\n  }\n} catch (e) {\n  console.log(' Error accessing webhook:', e);\n}\n\nfunction pickSystemPrompt() {\n  var personas = {\n    bob: 'You are Bob, a helpful and knowledgeable AI assistant.',\n    coach: 'You are a supportive coach.',\n    pm: 'You are a pragmatic project manager.'\n  };\n  try {\n    var body = $node['Webhook1'].first().json.body || {};\n    var direct = (typeof body.system_prompt_content === 'string') ? body.system_prompt_content.trim() : '';\n    if (direct) return direct;\n    var key = (body.persona_key || '').toString().toLowerCase();\n    if (personas[key]) return personas[key];\n  } catch (e) {}\n  return personas.bob;\n}\n\n// Get all merged items\nvar items = $input.all().map(function (it) { return it.json; });\n\n// Extract RAG context from system role messages\nvar ragContext = items\n  .filter(function (m) { return m.role === 'system'; })\n  .map(function (m) { return m.content; })\n  .join('\\n\\n');\n\n// Build final system message with RAG context\nvar finalSystem = pickSystemPrompt();\nif (ragContext) {\n  finalSystem += '\\n\\n--- Relevant Knowledge Base Context ---\\n' + ragContext;\n}\nfinalSystem += '\\n\\nFormatting rules: Use Markdown. Prefer short sections with headings when helpful. Use bullet lists for lists. Bold short labels. Use tables only when clearly beneficial. Keep responses concise and scannable.';\n\n// Start with system message\nvar messages = [{ role: 'system', content: finalSystem }];\n\n// Add all non-system messages (history and current input)\nvar nonSystem = items.filter(function (m) { return m.role !== 'system'; });\nmessages = messages.concat(nonSystem);\n\n// Check for vision usage in any message content\nvar usedVision = false;\nfor (var i = 0; i < nonSystem.length && !usedVision; i++) {\n  var c = nonSystem[i] && nonSystem[i].content;\n  if (Object.prototype.toString.call(c) === '[object Array]') {\n    for (var j = 0; j < c.length; j++) {\n      if (c[j] && c[j].type === 'image_url') {\n        usedVision = true;\n        break;\n      }\n    }\n  }\n}\n\n// Enhanced model selection logic\nvar model;\nconsole.log(' Final Model Selection Debug:');\nconsole.log('- frontendModel:', frontendModel);\nconsole.log('- usedVision:', usedVision);\nconsole.log('- modelType:', modelType);\nconsole.log('- DEFAULT_TEXT_MODEL:', DEFAULT_TEXT_MODEL);\nconsole.log('- VISION_MODEL:', VISION_MODEL);\n\nif (frontendModel) {\n  model = frontendModel;\n  console.log(' Using frontendModel:', model);\n  // For vision with OpenAI, override with vision model\n  if (usedVision && modelType === 'openai') {\n    model = VISION_MODEL;\n    console.log(' Vision override to:', model);\n  }\n  // Claude handles vision natively, so keep the selected model\n} else {\n  // Fallback to original logic\n  model = usedVision ? VISION_MODEL : DEFAULT_TEXT_MODEL;\n  console.log(' Using fallback model:', model, '(usedVision:', usedVision, ')');\n}\n\nconsole.log(' Final model decision:', model);\n\nreturn [{ \n  json: { \n    model: model, \n    messages: messages, \n    usedVision: usedVision,\n    frontendModel: frontendModel,\n    modelType: modelType,\n    routeTo: routeTo\n  } \n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        944,
        1904
      ],
      "id": "63c448f8-d563-4cc4-951b-aea428aaa77e",
      "name": "Build OpenAI Payload1"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        -1488,
        1440
      ],
      "id": "f591e353-3a26-4c26-bce5-898d97ba73de",
      "name": "Pre-flight Request"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "4b3cfa86-fdf5-4632-bbd3-84bb73ffb9bf",
              "leftValue": "={{ $json.rag_session_Id }}",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "notEmpty",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -816,
        2000
      ],
      "id": "d20d62f7-a565-4003-b17a-07c2bc498039",
      "name": "If Rag is Active"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT\n  original_content,\n  role,\n  embedding <=> $3::vector AS distance\nFROM\n  rag_store\nWHERE\n  project_id = $1 AND session_id = $2\nORDER BY\n  distance ASC\nLIMIT 3",
        "options": {
          "queryReplacement": "=[\n  {{ $('Preserve Current Inputs').first().json.project_Id }},\n  {{ $('Format Data for Vector Search').first().json.current_session_id_for_search }},\n  {{ $('Format Data for Vector Search').first().json.query_embedding_for_pg }}\n]"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -160,
        1728
      ],
      "id": "087bedda-ddfe-4646-ab98-e5e2b4f6d60d",
      "name": "Retrieve Committed Memory",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT\n  original_content,\n  role,\n  embedding <=> $3::vector AS distance\nFROM\n  rag_store\nWHERE\n  project_id = $1 AND session_id = $2\nORDER BY\n  distance ASC\nLIMIT 3",
        "options": {
          "queryReplacement": "=[\n  {{ $('Preserve Current Inputs').first().json.project_Id }},\n  {{ $('Format Data for Vector Search').first().json.current_session_id_for_search }},\n  {{ $('Format Data for Vector Search').first().json.query_embedding_for_pg }}\n]"
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -160,
        1920
      ],
      "id": "d890e232-6758-4879-9fbc-eb968600f3fd",
      "name": "Retrieve RAG Chunks",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "wnjdz7ukUBDZySLN",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        64,
        1824
      ],
      "id": "7cccdf8b-7879-4cd4-a56a-b77b843d83a2",
      "name": "Merge1",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        1600,
        1904
      ],
      "id": "2551f670-2a7b-48c4-86f5-839df07c0958",
      "name": "Respond to Webhook2"
    },
    {
      "parameters": {
        "multipleMethods": true,
        "path": "3c92075f-a856-439a-b70d-73f3c847f8fa",
        "responseMode": "responseNode",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Methods",
                "value": "GET, POST, OPTIONS"
              },
              {
                "name": "Access-Control-Allow-Headers",
                "value": "Content-Type, Authorization"
              },
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -1712,
        1520
      ],
      "id": "9b68cef4-a1a3-4cd5-a05f-46014c64f2e5",
      "name": "Webhook1",
      "webhookId": "58378ca2-9b32-4dfc-9e25-88b4fdce6ed3",
      "options": {
        "allowedOrigins": "*",
        "responseHeaders": {
          "entries": [
            {
              "name": "Access-Control-Allow-Methods",
              "value": "GET, POST, OPTIONS"
            },
            {
              "name": "Access-Control-Allow-Headers",
              "value": "Content-Type, Authorization"
            },
            {
              "name": "Access-Control-Allow-Origin",
              "value": "*"
            }
          ]
        }
      },
      "notes": "update 2"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "ai-router",
        "responseMode": "responseNode",
        "options": {}
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -1024,
        448
      ],
      "id": "397bd26b-39ee-48ed-be8b-613d7a6499a9",
      "name": "AI Router Webhook",
      "webhookId": "959cc516-c1a0-484c-9fe9-86ef40690e14"
    },
    {
      "parameters": {
        "jsCode": "// AI Router Input Processor\nvar body = $json.body || $json || {};\nvar selectedModel = body.selected_model || body.model || 'gpt-4o';\nvar messages = body.messages || [];\n\n// Determine if this is a Claude model - be very specific\nvar modelLower = selectedModel.toLowerCase();\nvar isClaudeModel = modelLower.includes('claude-') || modelLower.startsWith('claude');\n\n// For GPT models, ensure they are NOT Claude\nif (modelLower.includes('gpt') || modelLower.includes('openai')) {\n  isClaudeModel = false;\n}\n\n// Debug logging\nconsole.log('=== AI Router Debug ===');\nconsole.log('Selected Model:', selectedModel);\nconsole.log('Model Lower:', modelLower);\nconsole.log('Is Claude Model:', isClaudeModel);\nconsole.log('Expected Path:', isClaudeModel ? 'Claude' : 'OpenAI');\n\nreturn {\n  selectedModel: selectedModel,\n  messages: messages,\n  isClaudeModel: isClaudeModel,\n  originalPayload: body,\n  debug: {\n    selectedModel: selectedModel,\n    isClaudeModel: isClaudeModel,\n    modelLower: modelLower,\n    expectedPath: isClaudeModel ? 'Claude' : 'OpenAI'\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -816,
        448
      ],
      "id": "f7862c2c-0673-40c4-a895-ca2c32afdd8d",
      "name": "Input Processor"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": false,
            "leftValue": "",
            "typeValidation": "loose"
          },
          "conditions": [
            {
              "id": "claude-check",
              "leftValue": "={{ $json.selectedModel }}",
              "rightValue": "claude",
              "operator": {
                "type": "string",
                "operation": "contains",
                "caseSensitive": false
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -624,
        448
      ],
      "id": "884f3661-7824-409a-b55d-b7172325a818",
      "name": "Provider Check"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { \"model\": $json.selectedModel, \"messages\": $json.messages, \"max_tokens\": 4000 } }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -416,
        336
      ],
      "id": "3d087e7d-c614-48b1-8c2d-f7e1b725a0a6",
      "name": "OpenAI Request",
      "credentials": {
        "openAiApi": {
          "id": "4UBMhvVGj5H8Ehzq",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Claude Preprocessor v6.2 - Handles both HTTPS URLs and data URLs\nconsole.log('=== CLAUDE PREPROCESSOR v6.2 START ===');\nconsole.log('Input $json:', JSON.stringify($json, null, 2));\n\nvar originalPayload = $json.originalPayload || {};\nvar messages = $json.messages || [];\nvar selectedModel = $json.selectedModel;\n\nconsole.log('Extracted values:', {\n  originalPayload: originalPayload,\n  messagesCount: messages.length,\n  selectedModel: selectedModel\n});\n\n// Map frontend model names to ACTUAL Claude API model identifiers (2025)\nvar claudeModelMap = {\n  'claude-4-sonnet': 'claude-sonnet-4-20250514',\n  'claude-4.1-opus': 'claude-opus-4-1-20250805',\n  'claude-4-opus': 'claude-opus-4-20250514',\n  'claude-3.7-sonnet': 'claude-3-7-sonnet-20250219',\n  'claude-3-7-sonnet': 'claude-3-7-sonnet-20250219',\n  'claude-3.5-haiku': 'claude-3-5-haiku-20241022',\n  'claude-3-haiku': 'claude-3-haiku-20240307',\n  'claude-haiku-3.5': 'claude-3-5-haiku-20241022'\n};\n\n// Get the correct Claude API model identifier\nvar claudeApiModel = claudeModelMap[selectedModel] || 'claude-sonnet-4-20250514';\n\nconsole.log('=== Model Mapping ===');\nconsole.log('Frontend Model:', selectedModel);\nconsole.log('Mapped Claude Model:', claudeApiModel);\nconsole.log('Message count:', messages.length);\n\n// Convert OpenAI format to Claude format\nvar systemMessage = '';\nvar filteredMessages = [];\n\nconsole.log('=== Starting Message Processing ===');\nconsole.log('Raw messages:', JSON.stringify(messages, null, 2));\n\nfor (var i = 0; i < messages.length; i++) {\n  console.log('=== Processing Message', i, '===');\n  console.log('Message role:', messages[i].role);\n  console.log('Message content type:', typeof messages[i].content);\n  console.log('Is content array:', Array.isArray(messages[i].content));\n  \n  if (messages[i].role === 'system') {\n    systemMessage = messages[i].content || '';\n    console.log('Found system message, length:', systemMessage.length);\n  } else {\n    var messageContent = messages[i].content;\n    console.log('Processing non-system message with content:', typeof messageContent);\n    \n    // Handle image content conversion from OpenAI to Anthropic format\n    if (Array.isArray(messageContent)) {\n      console.log('Found multipart content with', messageContent.length, 'items');\n      var convertedContent = [];\n      for (var j = 0; j < messageContent.length; j++) {\n        var contentItem = messageContent[j];\n        console.log('Processing content item', j, 'type:', contentItem.type);\n        \n        if (contentItem.type === 'image_url') {\n          console.log('=== IMAGE PROCESSING START ===');\n          console.log('Image URL structure:', JSON.stringify(contentItem, null, 2));\n          \n          // Extract image URL from OpenAI format\n          var imageUrl = contentItem.image_url.url;\n          var base64Data = '';\n          var mediaType = 'image/jpeg'; // default\n          \n          console.log('Image URL type:', imageUrl.startsWith('data:') ? 'data URL' : 'HTTPS URL');\n          \n          // Handle different image URL formats\n          if (imageUrl.startsWith('data:')) {\n            // Extract base64 data from data URI\n            console.log('Processing data URL...');\n            var base64Start = imageUrl.indexOf(',') + 1;\n            base64Data = imageUrl.substring(base64Start);\n            \n            // Extract media type\n            var matches = imageUrl.match(/^data:([^;]+);base64,(.+)$/);\n            if (matches && matches.length === 3) {\n              mediaType = matches[1];\n              base64Data = matches[2];\n              console.log('Successfully extracted base64 data:', {\n                mediaType: mediaType,\n                dataLength: base64Data.length\n              });\n            } else {\n              // Fallback parsing for malformed data URLs\n              console.log('Using fallback parsing for data URL');\n              if (imageUrl.includes('image/png')) mediaType = 'image/png';\n              else if (imageUrl.includes('image/jpeg') || imageUrl.includes('image/jpg')) mediaType = 'image/jpeg';\n              else if (imageUrl.includes('image/webp')) mediaType = 'image/webp';\n              else if (imageUrl.includes('image/gif')) mediaType = 'image/gif';\n              console.log('Fallback extraction successful:', {\n                mediaType: mediaType,\n                dataLength: base64Data.length\n              });\n            }\n            \n            // Clean and validate base64 data\n            if (!base64Data || base64Data.length === 0) {\n              console.error('=== ERROR: Empty base64 data ===');\n              continue;\n            }\n            \n            // Clean base64 data\n            base64Data = base64Data.replace(/\\s+/g, '').replace(/\\n/g, '').replace(/\\r/g, '');\n            \n            // Ensure it's valid base64 characters only\n            base64Data = base64Data.replace(/[^A-Za-z0-9+/=]/g, '');\n            \n            // Pad if necessary\n            if (base64Data.length % 4 !== 0) {\n              var padding = 4 - (base64Data.length % 4);\n              if (padding < 4) {\n                base64Data += '='.repeat(padding);\n              }\n            }\n            \n            // Final validation\n            try {\n              var testData = base64Data.substring(0, Math.min(100, base64Data.length));\n              if (testData.length >= 4) {\n                atob(testData.substring(0, 4));\n              }\n              console.log('Base64 validation passed');\n            } catch (e) {\n              console.error('=== ERROR: Invalid base64 data ===');\n              console.error('Error:', e.message);\n              continue;\n            }\n            \n            // Ensure media type is supported\n            var supportedTypes = ['image/jpeg', 'image/png', 'image/webp', 'image/gif'];\n            if (!supportedTypes.includes(mediaType)) {\n              console.log('Unsupported media type, defaulting to image/jpeg:', mediaType);\n              mediaType = 'image/jpeg';\n            }\n            \n            console.log('=== IMAGE CONVERSION SUCCESS ===');\n            console.log('Final image data:', {\n              mediaType: mediaType,\n              base64Length: base64Data.length,\n              base64Preview: base64Data.substring(0, 20) + '...'\n            });\n            \n            // Create Claude format for base64\n            convertedContent.push({\n              type: 'image',\n              source: {\n                type: 'base64',\n                media_type: mediaType,\n                data: base64Data\n              }\n            });\n            \n          } else if (imageUrl.startsWith('http')) {\n            // For HTTPS URLs, pass directly to Claude (Claude supports URL format)\n            console.log('Processing HTTPS URL:', imageUrl.substring(0, 100) + '...');\n            \n            // Determine media type from URL extension\n            if (imageUrl.toLowerCase().includes('.png')) {\n              mediaType = 'image/png';\n            } else if (imageUrl.toLowerCase().includes('.jpg') || imageUrl.toLowerCase().includes('.jpeg')) {\n              mediaType = 'image/jpeg';\n            } else if (imageUrl.toLowerCase().includes('.webp')) {\n              mediaType = 'image/webp';\n            } else if (imageUrl.toLowerCase().includes('.gif')) {\n              mediaType = 'image/gif';\n            } else {\n              mediaType = 'image/jpeg'; // default fallback\n            }\n            \n            console.log('HTTPS URL will be passed directly to Claude:', {\n              url: imageUrl.substring(0, 50) + '...',\n              mediaType: mediaType\n            });\n            \n            // Create Claude format for URL (not base64)\n            convertedContent.push({\n              type: 'image',\n              source: {\n                type: 'url',\n                url: imageUrl\n              }\n            });\n            \n            console.log('=== IMAGE URL CONVERSION SUCCESS ===');\n            \n          } else {\n            console.error('=== UNSUPPORTED IMAGE FORMAT ===');\n            console.error('Only data: and https: URLs are supported. Received:', imageUrl.substring(0, 100));\n            continue; // Skip this image\n          }\n          \n        } else {\n          // Keep text content as-is\n          console.log('Keeping text content item:', contentItem.type || 'text');\n          convertedContent.push(contentItem);\n        }\n      }\n      messageContent = convertedContent;\n      console.log('Converted content array length:', convertedContent.length);\n    } else {\n      console.log('Content is not array, keeping as text:', typeof messageContent);\n    }\n    \n    filteredMessages.push({\n      role: messages[i].role,\n      content: messageContent\n    });\n    \n    console.log('Processed message ' + i + ':', {\n      role: messages[i].role,\n      contentType: Array.isArray(messageContent) ? 'multipart' : 'text',\n      contentLength: Array.isArray(messageContent) ? messageContent.length : messageContent.length\n    });\n  }\n}\n\n// Build Claude payload\nvar claudePayload = {\n  model: claudeApiModel,\n  max_tokens: originalPayload.max_tokens || 4000,\n  messages: filteredMessages\n};\n\nif (systemMessage) {\n  claudePayload.system = systemMessage;\n}\n\n// Final validation\nconsole.log('=== FINAL PAYLOAD VALIDATION ===');\nconsole.log('Model:', claudePayload.model);\nconsole.log('Messages count:', claudePayload.messages ? claudePayload.messages.length : 'undefined');\nconsole.log('System message present:', !!claudePayload.system);\n\nif (!claudePayload.model || !claudePayload.messages || claudePayload.messages.length === 0) {\n  console.error('=== CRITICAL ERROR: Invalid Claude payload ===');\n  console.error('Full payload:', JSON.stringify(claudePayload, null, 2));\n  throw new Error('Invalid payload: missing model or messages');\n}\n\n// Validate image content structure\nfor (var k = 0; k < claudePayload.messages.length; k++) {\n  var msg = claudePayload.messages[k];\n  if (Array.isArray(msg.content)) {\n    for (var l = 0; l < msg.content.length; l++) {\n      var content = msg.content[l];\n      if (content.type === 'image' && content.source) {\n        if (content.source.type === 'base64' && (!content.source.data || content.source.data.length === 0)) {\n          console.error('Empty image data in message', k, 'content', l);\n          throw new Error('Invalid image: empty base64 data');\n        }\n        if (content.source.type === 'url' && (!content.source.url || content.source.url.length === 0)) {\n          console.error('Empty image URL in message', k, 'content', l);\n          throw new Error('Invalid image: empty URL');\n        }\n        if (!content.source.media_type && content.source.type === 'base64') {\n          content.source.media_type = 'image/jpeg';\n        }\n        console.log('Image validation passed for message', k, 'content', l, 'type:', content.source.type);\n      }\n    }\n  }\n}\n\nconsole.log('=== PREPROCESSOR v6.2 SUCCESS ===');\nconsole.log('Final Claude Payload:', JSON.stringify(claudePayload, null, 2));\n\nreturn {\n  claudePayload: claudePayload,\n  selectedModel: selectedModel,\n  claudeApiModel: claudeApiModel\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -416,
        544
      ],
      "id": "1974b1ee-4182-4c9c-985b-70df1c0040d8",
      "name": "Claude Preprocessor"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.anthropic.com/v1/messages",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "anthropicApi",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            },
            {
              "name": "anthropic-version",
              "value": "2023-06-01"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json.claudePayload }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -208,
        544
      ],
      "id": "c0b14801-84fe-48c7-bf1d-b2b712244278",
      "name": "Claude Request",
      "credentials": {
        "anthropicApi": {
          "id": "4t4ANeac7zJTypWv",
          "name": "Anthropic account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Normalize OpenAI Response\nvar response = $json;\nvar selectedModel = 'unknown';\n\n// Try to get the selected model from the input processor\ntry {\n  var inputProcessorData = $('Input Processor').first();\n  if (inputProcessorData && inputProcessorData.json) {\n    selectedModel = inputProcessorData.json.selectedModel || 'unknown';\n  }\n} catch(e) {\n  // Fallback: try to get from response or use default\n  selectedModel = response.model || 'gpt-4o';\n}\n\nvar normalizedResponse = {};\n\nif (response.choices && response.choices[0] && response.choices[0].message) {\n  normalizedResponse = {\n    success: true,\n    provider: 'openai',\n    model: selectedModel,\n    content: response.choices[0].message.content,\n    usage: response.usage || {}\n  };\n} else {\n  normalizedResponse = {\n    success: false,\n    provider: 'openai',\n    model: selectedModel,\n    error: 'Invalid OpenAI response format',\n    raw_response: response\n  };\n}\n\nreturn normalizedResponse;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -208,
        336
      ],
      "id": "f587ad2f-09fe-4d2f-b49e-f9c5250cbc66",
      "name": "OpenAI Normalizer"
    },
    {
      "parameters": {
        "jsCode": "// Normalize Claude Response\nvar response = $json;\nvar selectedModel = 'unknown';\nvar displayModel = 'claude-sonnet-4';\n\nconsole.log('=== Claude Normalizer Debug ===');\nconsole.log('Received response:', JSON.stringify(response, null, 2));\n\n// Try to get the selected model from the claude preprocessor\ntry {\n  var claudeData = $('Claude Preprocessor').first();\n  if (claudeData && claudeData.json) {\n    selectedModel = claudeData.json.selectedModel || 'unknown';\n    var claudeApiModel = claudeData.json.claudeApiModel;\n    \n    // Use a clean display name based on the API model\n    if (claudeApiModel && claudeApiModel.includes('claude-sonnet-4')) {\n      displayModel = 'claude-4-sonnet';\n    } else if (claudeApiModel && claudeApiModel.includes('claude-opus-4-1')) {\n      displayModel = 'claude-4.1-opus';\n    } else if (claudeApiModel && claudeApiModel.includes('claude-opus-4')) {\n      displayModel = 'claude-4-opus';\n    } else if (claudeApiModel && claudeApiModel.includes('claude-3-7-sonnet')) {\n      displayModel = 'claude-3.7-sonnet';\n    } else if (claudeApiModel && claudeApiModel.includes('claude-3-5-haiku')) {\n      displayModel = 'claude-3.5-haiku';\n    } else if (claudeApiModel && claudeApiModel.includes('claude-3-haiku')) {\n      displayModel = 'claude-3-haiku';\n    } else {\n      displayModel = selectedModel; // fallback to frontend name\n    }\n  }\n} catch(e) {\n  console.log('Error getting claude data:', e.message);\n  selectedModel = 'claude-4-sonnet';\n  displayModel = 'claude-4-sonnet';\n}\n\nconsole.log('Frontend Model:', selectedModel);\nconsole.log('Display Model:', displayModel);\n\nvar normalizedResponse = {};\n\nif (response.content && response.content[0] && response.content[0].text) {\n  normalizedResponse = {\n    success: true,\n    provider: 'claude',\n    model: displayModel,\n    content: response.content[0].text,\n    usage: response.usage || {}\n  };\n  console.log('Successfully normalized Claude response');\n} else {\n  console.log('Failed to normalize Claude response - invalid format');\n  normalizedResponse = {\n    success: false,\n    provider: 'claude',\n    model: displayModel,\n    error: 'Invalid Claude response format',\n    raw_response: response\n  };\n}\n\nconsole.log('Final normalized response:', JSON.stringify(normalizedResponse, null, 2));\nreturn normalizedResponse;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -16,
        544
      ],
      "id": "7f6b4dce-b3a0-44bb-a152-5d244caf0a32",
      "name": "Claude Normalizer"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.2,
      "position": [
        192,
        448
      ],
      "id": "f2042055-3fcd-44c2-87fc-f8e9c320f5df",
      "name": "Response"
    },
    {
      "parameters": {
        "content": "## AI Router",
        "height": 80,
        "width": 160
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -704,
        320
      ],
      "typeVersion": 1,
      "id": "198267cb-0f3f-4e06-ade9-69498284becb",
      "name": "Sticky Note5"
    }
  ],
  "pinData": {
    "Trigger: Receive Session ID": [
      {
        "json": {
          "headers": {
            "host": "mhcmike.app.n8n.cloud",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
            "content-length": "3128",
            "accept": "application/json",
            "accept-encoding": "gzip, br",
            "accept-language": "en-US,en;q=0.9",
            "cdn-loop": "cloudflare; loops=1; subreqs=1",
            "cf-connecting-ip": "72.219.130.70",
            "cf-ew-via": "15",
            "cf-ipcountry": "US",
            "cf-ray": "97aefde2651d0fb8-LAX",
            "cf-visitor": "{\"scheme\":\"https\"}",
            "cf-worker": "n8n.cloud",
            "content-type": "application/json",
            "origin": "http://127.0.0.1:5500",
            "priority": "u=1, i",
            "referer": "http://127.0.0.1:5500/",
            "sec-ch-ua": "\"Chromium\";v=\"140\", \"Not=A?Brand\";v=\"24\", \"Google Chrome\";v=\"140\"",
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": "\"Windows\"",
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "cross-site",
            "x-forwarded-for": "72.219.130.70, 172.70.210.225",
            "x-forwarded-host": "mhcmike.app.n8n.cloud",
            "x-forwarded-port": "443",
            "x-forwarded-proto": "https",
            "x-forwarded-server": "traefik-prod-users-gwc-34-f494454b5-f88k9",
            "x-is-trusted": "yes",
            "x-real-ip": "72.219.130.70"
          },
          "params": {},
          "query": {},
          "body": {
            "chat_session_id": "session_6ige5bcxg",
            "project_id": "rec_d2u53dtqrj67jthrfi0g",
            "rag_session_id": "8d6becfa-d861-45c9-ac43-2078e93ef3fa",
            "history": [
              {
                "role": "assistant",
                "content": "Project \"9/6/25 Test 1\" created successfully. Persona: Developer Assistant (A)."
              },
              {
                "role": "user",
                "content": "Hello my name is mike. I'm doing some testing this morning. I'll be making some statements and then adding them to a memory function I have designed. I'll close this session, restart it and see if you can remember these statements. Are you ready?"
              },
              {
                "role": "assistant",
                "content": "Hello Mike! Yes, I'm ready to help with your testing.\nJust to set clear expectations: I don't have the ability to retain information between separate chat sessions on my own. Each time you start a new conversation with me, I begin with a fresh slate and no memory of previous interactions.\nHowever, I understand you're testing a memory function you've designed, which sounds like it might work by:\n\nStoring information from our current session\nMaking that information available to me in future sessions (perhaps through the knowledge base context)\n\nI'm ready to participate in your test! Feel free to share the statements you'd like to test with your memory system.\nmodel?"
              },
              {
                "role": "user",
                "content": "Here we go...my truck is blue"
              },
              {
                "role": "assistant",
                "content": "Got it, Mike! I've noted that your truck is blue.\nI'm ready for any additional statements you'd like to share for your memory function testing. Just let me know when you have more information to add or when you're ready to move to the next phase of your test.\nmodel?"
              },
              {
                "role": "user",
                "content": "the truck is a Ford F350"
              },
              {
                "role": "assistant",
                "content": "Perfect! I've now noted:\n\nYour truck is blue\nYour truck is a Ford F350\n\nSo you have a blue Ford F350. Got it recorded for your memory function test!\nReady for any additional statements or whenever you're ready to proceed to the next phase of testing.\nmodel?"
              },
              {
                "role": "user",
                "content": "I have a green fan on my desk"
              },
              {
                "role": "assistant",
                "content": "Noted, Mike! I now have these details recorded for your memory function test:\n\nYour truck is blue\nYour truck is a Ford F350 \nYou have a green fan on your desk\n\nThree pieces of information logged! Ready for more statements or whenever you want to move to the next phase of your memory system testing.\nmodel?"
              },
              {
                "role": "user",
                "content": "ok, thanks. I'm going to commit this session to memory (a RAG Vector database), stop/restart and check if you can remember."
              },
              {
                "role": "assistant",
                "content": "Sounds good, Mike! I understand you're testing your memory system by:\n\nCommitting this session to your RAG Vector database\nStopping/restarting the system\nTesting recall to see if I can remember the details we discussed\n\nThe information we covered for your test:\n\nYour truck is blue\nYour truck is a Ford F350\nYou have a green fan on your desk\n\nGo ahead and run your commit process. I'll be ready to demonstrate whether the memory system successfully retrieves this information when you restart and test recall!\nmodel?"
              }
            ],
            "database_branch": "main",
            "xata_branch": "main"
          },
          "webhookUrl": "https://mhcmike.app.n8n.cloud/webhook/6c1ce608-2f7a-457b-9afc-f0be5ef4bd4c",
          "executionMode": "production"
        }
      }
    ],
    "Webhook3": [
      {
        "json": {
          "headers": {
            "host": "n8n.srv997771.hstgr.cloud",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
            "content-length": "72",
            "accept": "*/*",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "en-US,en;q=0.9",
            "cache-control": "no-cache",
            "content-type": "application/json",
            "origin": "http://127.0.0.1:5500",
            "pragma": "no-cache",
            "priority": "u=1, i",
            "referer": "http://127.0.0.1:5500/",
            "sec-ch-ua": "\"Chromium\";v=\"140\", \"Not=A?Brand\";v=\"24\", \"Google Chrome\";v=\"140\"",
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": "\"Windows\"",
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "cross-site",
            "x-forwarded-for": "72.219.130.70",
            "x-forwarded-host": "n8n.srv997771.hstgr.cloud",
            "x-forwarded-port": "443",
            "x-forwarded-proto": "https",
            "x-forwarded-server": "0e4effe4917a",
            "x-real-ip": "72.219.130.70"
          },
          "params": {},
          "query": {},
          "body": {
            "action": "list_projects",
            "database_branch": "main",
            "xata_branch": "main"
          },
          "webhookUrl": "https://n8n.srv997771.hstgr.cloud/webhook/a61a290c-d8e5-4c04-980a-4ebb415a21e4",
          "executionMode": "production"
        }
      }
    ],
    "Webhook": [
      {
        "json": {
          "headers": {
            "host": "n8n.srv997771.hstgr.cloud",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
            "content-length": "108",
            "accept": "*/*",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "en-US,en;q=0.9",
            "content-type": "application/json",
            "origin": "http://127.0.0.1:5500",
            "priority": "u=1, i",
            "referer": "http://127.0.0.1:5500/",
            "sec-ch-ua": "\"Chromium\";v=\"140\", \"Not=A?Brand\";v=\"24\", \"Google Chrome\";v=\"140\"",
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": "\"Windows\"",
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "cross-site",
            "x-forwarded-for": "72.219.130.70",
            "x-forwarded-host": "n8n.srv997771.hstgr.cloud",
            "x-forwarded-port": "443",
            "x-forwarded-proto": "https",
            "x-forwarded-server": "0e4effe4917a",
            "x-real-ip": "72.219.130.70"
          },
          "params": {},
          "query": {},
          "body": {
            "projectName": "9/8/25 Test v1",
            "persona_key": "dev_assistant",
            "database_branch": "main",
            "xata_branch": "main"
          },
          "webhookUrl": "https://n8n.srv997771.hstgr.cloud/webhook/d0b91f11-487b-441f-80a3-17edd5a703db",
          "executionMode": "production"
        }
      }
    ],
    "Webhook2": [
      {
        "json": {
          "headers": {
            "host": "n8n.srv997771.hstgr.cloud",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
            "content-length": "157",
            "accept": "*/*",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "en-US,en;q=0.9",
            "content-type": "application/json",
            "origin": "http://127.0.0.1:5500",
            "priority": "u=1, i",
            "referer": "http://127.0.0.1:5500/",
            "sec-ch-ua": "\"Chromium\";v=\"140\", \"Not=A?Brand\";v=\"24\", \"Google Chrome\";v=\"140\"",
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": "\"Windows\"",
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "cross-site",
            "x-forwarded-for": "72.219.130.70",
            "x-forwarded-host": "n8n.srv997771.hstgr.cloud",
            "x-forwarded-port": "443",
            "x-forwarded-proto": "https",
            "x-forwarded-server": "0e4effe4917a",
            "x-real-ip": "72.219.130.70"
          },
          "params": {},
          "query": {},
          "body": {
            "projectName": "9/8/25 Test Bob 700k v2",
            "jsonUrl": "https://storage.googleapis.com/mhcmike-ai-chat-interface/Bob_Original_700k.json",
            "persona_key": "bob_mhc"
          },
          "webhookUrl": "https://n8n.srv997771.hstgr.cloud/webhook/303639ed-a3e2-4eae-b406-16e1c6200a81",
          "executionMode": "production"
        }
      }
    ],
    "AI Router Webhook": [
      {
        "json": {
          "headers": {
            "host": "n8n.srv997771.hstgr.cloud",
            "user-agent": "axios/1.8.3",
            "content-length": "3960",
            "accept": "application/json,text/html,application/xhtml+xml,application/xml,text/*;q=0.9, image/*;q=0.8, */*;q=0.7",
            "accept-encoding": "gzip, compress, deflate, br",
            "content-type": "application/json",
            "x-forwarded-for": "172.18.0.1",
            "x-forwarded-host": "n8n.srv997771.hstgr.cloud",
            "x-forwarded-port": "443",
            "x-forwarded-proto": "https",
            "x-forwarded-server": "0910137f0700",
            "x-real-ip": "172.18.0.1"
          },
          "params": {},
          "query": {},
          "body": {
            "selected_model": "claude-4-sonnet",
            "messages": [
              {
                "role": "system",
                "content": "You are Bob, a helpful and knowledgeable AI assistant.\n\n--- Relevant Knowledge Base Context ---\n--- CONTEXT FROM KNOWLEDGE BASE ---\n{\n    \"facts\": [\n        \"The user is conducting a test on their modified user interface, which now preserves session dialogue content.\",\n        \"The user described a system utilizing a 'commit to memory' button that activates when there's conversation content in the chat.\",\n        \"The user explained a RAG (Retrieval-Augmented Generation) system involving 'conversation_history' and 'rag_store' tables to manage conversation continuity and context.\"\n    ]\n}\n\nFormatting rules: Use Markdown. Prefer short sections with headings when helpful. Use bullet lists for lists. Bold short labels. Use tables only when clearly beneficial. Keep responses concise and scannable."
              },
              {
                "role": "user",
                "content": "**I'm Claude 3.5 Sonnet** - but I don't have access to my specific version number or build details from my current context.\n\n**What I can tell you:**\n- **Model family:** Claude 3.5 Sonnet (Anthropic)\n- **Capabilities:** Text, image analysis, code, reasoning\n- **Version specifics:** Not visible to me in this session\n\n**To get exact version info, you'd need to:**\n- Check your API configuration/dashboard\n- Look at request headers if using API\n- Check the platform documentation where you're running me\n\n**For your testing purposes:** I'm consistently handling your image uploads and analysis without issues, regardless of the specific build version.\n\n**Ready to continue the image testing sequence!**"
              },
              {
                "role": "user",
                "content": "OK Bob here's another image coming at you"
              },
              {
                "role": "user",
                "content": "**Test Image #4 - Labrador Retriever:**\n\n**Features:**\n- **Golden coat** - classic Labrador color\n- **Outdoor scene** - standing on beach with mountains in background\n- **Alert posture** - focused and attentive\n- **Natural lighting** - soft, even tones\n\n**Technical status:**\n- **Upload:** Successful\n- **Display:** Clean render\n- **Quality:** Excellent detail\n- **UI:** No issues detected\n\n**Everything's looking great!** Let me know when you're ready for the next one or if anything specific needs attention."
              },
              {
                "role": "user",
                "content": "Please tell me your model and version number"
              },
              {
                "role": "user",
                "content": "I currently don't have access to my specific model version number or build details within this session. However, I operate under the **Claude 3.5 Sonnet** model family, developed by Anthropic. For precise version details:\n\n- **Check your API interface/dashboard** if applicable.\n- **Review request headers** of your API calls.\n- **Consult your platform documentation** if you're using a specific service to run these tests.\n\nIf there's anything else you need or any specific issues arise during your image tests, feel free to let me know!"
              },
              {
                "role": "user",
                "content": ""
              },
              {
                "role": "user",
                "content": [
                  {
                    "type": "text",
                    "text": ""
                  },
                  {
                    "type": "image_url",
                    "image_url": {
                      "url": "https://storage.googleapis.com/mhcmike-ai-chat-interface/Images/general/unknown/2025-09-28T19%3A50%3A08.998Z_2025-09-28_12-50-05.PNG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vertex-ai-user-for-n8n%40persistant-ai-chat.iam.gserviceaccount.com%2F20250928%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250928T195009Z&X-Goog-Expires=3600&X-Goog-SignedHeaders=host&X-Goog-Signature=52a421c21c8fe2e0619c456ea1fdcb80e88bb6ce1045973bbab569170a7f64718708e095c5639071e8b9385f9ca9adb94ac215641ef69ee0dbc7cb88f24b260849911c855e2cdfd3fee5bc8c2d069bec513ed838551a1748347a73d22f417ec52028f5a2fb43734e4e1d4cd00a52ddb2efcc1eb58f4176a407e4431b4de4748f11b1b4961bd645cd2fbaa5db19af0a6a0b80933e081fdf98b9e2384dbf2bf0e81b0da91a42035414136207ae8e92d4e8ea106961a06e168f0146ebce48c12944c51253e0aa434b91549eb576e5a7508d3c88f8baf630005a7d12cce4d0c9c5eb89806d63d98f6999a5e662d0ed269d335da60fdf856b5b27428d579406566135",
                      "detail": "auto"
                    }
                  }
                ]
              }
            ]
          },
          "webhookUrl": "https://n8n.srv997771.hstgr.cloud/webhook/ai-router",
          "executionMode": "production"
        }
      }
    ],
    "Webhook1": []
  },
  "connections": {
    "DB: Get Conversation History": {
      "main": [
        [
          {
            "node": "Calculate New High-Water Mark",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format: Assemble Transcript": {
      "main": [
        [
          {
            "node": "Build: OpenAI Request Body",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build: OpenAI Request Body": {
      "main": [
        [
          {
            "node": "Execute: OpenAI API Call",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Execute: OpenAI API Call": {
      "main": [
        [
          {
            "node": "Set: Extracted Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DB: Get High-Water Mark": {
      "main": [
        [
          {
            "node": "Set: Watermark Value",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set: Watermark Value": {
      "main": [
        [
          {
            "node": "DB: Get Conversation History",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set: Extracted Summary": {
      "main": [
        [
          {
            "node": "Chunk: Distilled Summary",
            "type": "main",
            "index": 0
          },
          {
            "node": "Count Facts",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Count Facts": {
      "main": [
        [
          {
            "node": "Merge2",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Chunk: Distilled Summary": {
      "main": [
        [
          {
            "node": "API: Create Embeddings (HTTP) node",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DB: Insert Embedding1": {
      "main": [
        [
          {
            "node": "DB: Update High-Water Mark",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DB: Update High-Water Mark": {
      "main": [
        [
          {
            "node": "Merge2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "IF: New Messages Found?": {
      "main": [
        [
          {
            "node": "Format: Assemble Transcript",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Calculate New High-Water Mark": {
      "main": [
        [
          {
            "node": "IF: New Messages Found?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "API: Create Embeddings (HTTP) node": {
      "main": [
        [
          {
            "node": "Merge Embedding with Original Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Embedding with Original Data": {
      "main": [
        [
          {
            "node": "DB: Insert Embedding1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Project ID": {
      "main": [
        [
          {
            "node": "DB: Get High-Water Mark",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Trigger: Receive Session ID": {
      "main": [
        [
          {
            "node": "Get Project ID",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge2": {
      "main": [
        [
          {
            "node": "Respond to Webhook6",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Postgres1": {
      "main": [
        [
          {
            "node": "Respond to Webhook5",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Project List": {
      "main": [
        [
          {
            "node": "If1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code2": {
      "main": [
        [
          {
            "node": "Postgres1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If1": {
      "main": [
        [
          {
            "node": "Code3",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Code2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook3": {
      "main": [
        [
          {
            "node": "Get Project List",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code3": {
      "main": [
        [
          {
            "node": "Respond to Webhook4",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook": {
      "main": [
        [
          {
            "node": "Check if Project Exists",
            "type": "main",
            "index": 0
          },
          {
            "node": "If",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Postgres": {
      "main": [
        [
          {
            "node": "Generate Thread ID",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If": {
      "main": [
        [
          {
            "node": "Respond to Webhook1",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Code",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code": {
      "main": [
        [
          {
            "node": "Postgres",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Success Response": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Thread ID": {
      "main": [
        [
          {
            "node": "Save Thread ID to Project",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save Thread ID to Project": {
      "main": [
        [
          {
            "node": "Prepare Success Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create Embedding1": {
      "main": [
        [
          {
            "node": "Merge Embedding with Chunk Data1",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Format Vector for Postgres1": {
      "main": [
        [
          {
            "node": "Insert Embedding to Xata1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert Embedding to Xata1": {
      "main": [
        [
          {
            "node": "Prepare Final Response1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare for DB1": {
      "main": [
        [
          {
            "node": "Format Vector for Postgres1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTTP Request GCS Fetch1": {
      "main": [
        [
          {
            "node": "Chunk Code1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Chunk Code1": {
      "main": [
        [
          {
            "node": "Filter1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Embedding with Chunk Data1": {
      "main": [
        [
          {
            "node": "Prepare for DB1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Filter1": {
      "main": [
        [
          {
            "node": "Create Embedding1",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge Embedding with Chunk Data1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create Project Entry1": {
      "main": [
        [
          {
            "node": "HTTP Request GCS Fetch1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Final Response1": {
      "main": [
        [
          {
            "node": "Respond to Webhook3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook2": {
      "main": [
        [
          {
            "node": "Create Project Entry1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTTP Request to AI Router": {
      "main": [
        [
          {
            "node": "Code1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code1": {
      "main": [
        [
          {
            "node": "Postgres2 (Save AI Reply)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Respond to Webhook2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format History for AI": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create Query Embedding": {
      "main": [
        [
          {
            "node": "Format Data for Vector Search",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Data for Vector Search": {
      "main": [
        [
          {
            "node": "Retrieve Committed Memory",
            "type": "main",
            "index": 0
          },
          {
            "node": "Retrieve RAG Chunks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Session ID & Input": {
      "main": [
        [
          {
            "node": "Save User Message to History",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get Recent history": {
      "main": [
        [
          {
            "node": "Format Recent History",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Recent History": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Save User Message to History": {
      "main": [
        [
          {
            "node": "Preserve Current Inputs",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Preserve Current Inputs": {
      "main": [
        [
          {
            "node": "Format Current Input",
            "type": "main",
            "index": 0
          },
          {
            "node": "If Rag is Active",
            "type": "main",
            "index": 0
          },
          {
            "node": "Config: Auto-Commit Enabled",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Config: Auto-Commit Enabled": {
      "main": [
        [
          {
            "node": "If Remember Intent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If Remember Intent": {
      "main": [
        [
          {
            "node": "Auto Commit via Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Context consolidator": {
      "main": [
        [
          {
            "node": "Format History for AI",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge": {
      "main": [
        [
          {
            "node": "Build OpenAI Payload1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Current Input": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 2
          }
        ]
      ]
    },
    "Build OpenAI Payload1": {
      "main": [
        [
          {
            "node": "HTTP Request to AI Router",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If Rag is Active": {
      "main": [
        [
          {
            "node": "Create Query Embedding",
            "type": "main",
            "index": 0
          },
          {
            "node": "Get Recent history",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Get Recent history",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Retrieve Committed Memory": {
      "main": [
        [
          {
            "node": "Merge1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Retrieve RAG Chunks": {
      "main": [
        [
          {
            "node": "Merge1",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge1": {
      "main": [
        [
          {
            "node": "RAG Context consolidator",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Webhook1": {
      "main": [
        [
          {
            "node": "Pre-flight Request",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Get Session ID & Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "AI Router Webhook": {
      "main": [
        [
          {
            "node": "Input Processor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Input Processor": {
      "main": [
        [
          {
            "node": "Provider Check",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Provider Check": {
      "main": [
        [
          {
            "node": "Claude Preprocessor",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "OpenAI Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Request": {
      "main": [
        [
          {
            "node": "OpenAI Normalizer",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claude Preprocessor": {
      "main": [
        [
          {
            "node": "Claude Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claude Request": {
      "main": [
        [
          {
            "node": "Claude Normalizer",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Normalizer": {
      "main": [
        [
          {
            "node": "Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claude Normalizer": {
      "main": [
        [
          {
            "node": "Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "callerPolicy": "workflowsFromSameOwner"
  },
  "versionId": "04fad775-ea64-48b0-9869-f7d751432ffb",
  "meta": {
    "instanceId": "8fe1de58704fdaeb90ba65ff6b0aed2cf5efcb6cafc89641fb03be73646f227e"
  },
  "id": "rILTGpLhA1zsprX8",
  "tags": []
}