{"name":"System Documentation: Main Chat Workflow","nodes":[{"parameters":{"method":"POST","url":"https://api.openai.com/v1/chat/completions","authentication":"predefinedCredentialType","nodeCredentialType":"openAiApi","sendHeaders":true,"headerParameters":{"parameters":[{"name":"Content-Type","value":"application/json"}]},"sendBody":true,"specifyBody":"json","jsonBody":"={{ $json }}","options":{}},"type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[3380,900],"id":"96fad163-346a-40fc-bd7a-b5131b9741f0","name":"HTTP Request1","credentials":{"googleApi":{"id":"ibcCVTnTEjkd4DaY","name":"Google Service Account account"},"openAiApi":{"id":"coeze5tlOwYaVsLb","name":"OpenAi account"}}},{"parameters":{"mode":"runOnceForEachItem","jsCode":"// Code1 - Mode: Run Once for EACH Item\n// Input: A single item from HTTP Request1 (OpenAI API response)\n// Output: An object { reply: \"...\", toast?: \"...\" }\n\nconst openAIResponse = $input.item.json; \nlet replyText = \"Error: Could not extract reply from AI response in Code1.\"; \nlet toast;\n\nif (openAIResponse \u0026\u0026 \n    openAIResponse.choices \u0026\u0026 \n    openAIResponse.choices.length \u003e 0 \u0026\u0026\n    openAIResponse.choices[0].message \u0026\u0026\n    typeof openAIResponse.choices[0].message.content === \u0027string\u0027) {\n  \n  let rawReply = openAIResponse.choices[0].message.content;\n  \n  // --- ADD CLEANING \u0026 TRUNCATION ---\n  // Remove non-printable ASCII except common whitespace.\n  let cleanedReply = rawReply.replace(/[^\\x20-\\x7E\\n\\r\\t]+/g, \u0027\u0027); \n\n  const maxLength = 15000;\n  if (cleanedReply.length \u003e maxLength) {\n    cleanedReply = cleanedReply.substring(0, maxLength) + \"... (truncated)\";\n  }\n  replyText = cleanedReply;\n  // --- END CLEANING \u0026 TRUNCATION ---\n\n  // If auto-commit ran this turn, append a short memory status line to the reply and expose a toast field.\n  try {\n    const autoNode = $node[\"Auto Commit via Webhook\"]?.first();\n    const statusLine = autoNode?.json?.status_line;\n    const toastLine = autoNode?.json?.toast;\n    if (statusLine) {\n      replyText += \"\\n\\n\" + statusLine;\n    }\n    if (toastLine) { toast = toastLine; }\n  } catch (e) { /* ignore if node didn\u0027t run */ }\n\n  console.log(\"Code1 Extracted AND PROCESSED AI reply:\", replyText);\n\n} else {\n  console.error(\"Code1: Could not extract AI reply from OpenAI response. Input item.json was:\", JSON.stringify(openAIResponse, null, 2));\n}\n\nconst out = { reply: replyText };\nif (toast) out.toast = toast;\nreturn out;"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[3600,900],"id":"9f7d1977-7e1d-4939-a5bd-b8e2cb6f0311","name":"Code1"},{"parameters":{"jsCode":"// --- Prepare Consolidated RAG Context v1.0 ---\n// This script takes the single block of consolidated RAG text\n// and formats it correctly for the final prompt.\n\n// Get the combined context from the previous node\u0027s output.\nconst combinedContext = $input.item.json.combined_rag_context;\n\n// Create a single \"context\" object that the Merge node can use.\n// We will assign it the role of \"system\" to indicate it\u0027s background info.\nconst formattedRagContext = {\n  role: \u0027system\u0027,\n  content: `--- CONTEXT FROM KNOWLEDGE BASE ---\\n${combinedContext}`\n};\n\n// Return a single item containing this formatted object.\nreturn [{\n  json: formattedRagContext\n}];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[2720,820],"id":"941efc66-5aaa-42cc-964d-4ea2cf3bc59f","name":"Format History for AI","alwaysOutputData":true},{"parameters":{"method":"POST","url":"https://api.openai.com/v1/embeddings","authentication":"predefinedCredentialType","nodeCredentialType":"openAiApi","sendHeaders":true,"headerParameters":{"parameters":[{"name":"Content-Type","value":"application/json"}]},"sendBody":true,"specifyBody":"json","jsonBody":"={{\n  {\n    \"model\": \"text-embedding-3-small\",\n    \"input\": $(\u0027Get Session ID \u0026 Input\u0027).item.json.chatInput\n  }\n}}","options":{"response":{"response":{"responseFormat":"json"}}}},"type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[1620,820],"id":"8b2b53f6-afe3-4e2e-97e9-6bdb8973d98e","name":"Create Query Embedding","credentials":{"openAiApi":{"id":"jSjNufUyszBKKWwb","name":"OpenAi account 2"}}},{"parameters":{"jsCode":"// This node receives two inputs.\n// Input 0: From \"Create Query Embedding\"\n// Input 1: From the initial Webhook trigger\n\n// Get the embedding from the embedding creation node (input 0)\nconst embeddingData = $input.all(0)[0].json;\nconst embedding = embeddingData.data[0].embedding;\n\n// Get the data directly from the initial Webhook trigger node by its name\nconst triggerData = $(\u0027Webhook1\u0027).first().json;\n\n// Get the RAG session ID from the webhook\u0027s parsed JSON body\nconst ragSessionId = triggerData.body.rag_session_id;\n\n// Format the embedding vector into the string format for pgvector.\nconst query_embedding_for_pg = \u0027[\u0027 + embedding.join(\u0027,\u0027) + \u0027]\u0027;\n\n// Create the final output object.\n// It\u0027s crucial that this object has a \u0027json\u0027 property containing our data.\nreturn {\n  json: {\n    query_embedding_for_pg: query_embedding_for_pg,\n    current_session_id_for_search: ragSessionId\n  }\n};"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[1840,820],"id":"55256161-363d-4fb2-a26b-aad689d1ab0e","name":"Format Data for Vector Search"},{"parameters":{"jsCode":"const body = $input.item.json.body;\n\nif (!body.chat_session_id || typeof body.chat_session_id !== \u0027string\u0027) {\n  throw new Error(\"The \u0027chat_session_id\u0027 field is missing or invalid in the webhook body.\");\n}\nif (!body.chatInput \u0026\u0026 !body.imageData) {\n  throw new Error(\"The \u0027chatInput\u0027 field and \u0027imageData\u0027 are missing from the webhook body.\");\n}\n\n// Pass ALL required IDs and imageData downstream.\nreturn [{\n  json: {\n    chatInput: body.chatInput,\n    imageData: body.imageData || null,\n    session_Id: body.chat_session_id,\n    rag_session_Id: body.rag_session_id,\n    project_Id: body.project_id\n  }\n}];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[740,640],"id":"4e5a7ea0-70fc-4167-aa07-bd25a1143fb4","name":"Get Session ID \u0026 Input"},{"parameters":{"content":"## System Documentation: Main Chat Workflow\n\n\nWorkflow Name: Main Chat Workflow for Documentation\nVersion: As of file provided\nProject Lead: Mike Holland\nSystem Architect: Gemini Pro\n1. Overall Goal\nThis n8n workflow serves as the central orchestration layer for the Chat 8 user interface. Its primary purpose is to receive user input, intelligently enrich it with multiple forms of contextâ€”long-term memory from a RAG store and short-term conversational historyâ€”and generate a context-aware response from a large language model. It also includes a new feature for automatically committing \"remember\" intents to memory.\n2. Key Components \u0026 Architecture\nThe system is comprised of several key components that work in concert:\nFrontend: The Chat 8 UI (Chat_8_V7.html) captures user input and displays the final response.\nBackend Orchestration: This n8n workflow manages the entire data flow and logic.\nLong-Term Memory: A Postgres database containing the rag_store for vectorized knowledge.\nShort-Term Memory: The same Postgres database, using the conversation_history table for recent conversational turns.\nThe workflow\u0027s core architecture is a multi-branch parallel processing system. After initial input, it splits into distinct paths to gather different types of context. The final assembly is now handled directly inside \"Build OpenAI Payload1\" without a final Merge node, removing race conditions.\n3. Step-by-Step Data Flow\nThe workflow executes in several distinct phases:\nPhase 1: Ingestion and Preparation\nWebhook1: The workflow is triggered by a POST request from the UI. It is configured to handle pre-flight OPTIONS requests for CORS compatibility.\nGet Session ID \u0026 Input: A Code node parses the incoming request body. It validates that chat_session_id and chatInput are present and transforms all key IDs to a consistent camelCase format (session_Id, rag_session_Id, project_Id) for use within the workflow.\nSave User Message to History: A Postgres node immediately saves the user\u0027s message to the conversation_history table, ensuring a complete and persistent log of the interaction.\nPreserve Current Inputs: A critical Set node creates a stable, preserved copy of the key inputs (session_Id, chatInput, rag_session_Id, etc.). This node acts as a central hub, providing a reliable data source for all subsequent parallel branches.\nPhase 2: Auto-Commit Branch (Side Process)\nRunning in parallel to the main chat logic.\nConfig: Auto-Commit Enabled: A Set node acts as a feature flag, enabling the auto-commit functionality.\nIf Remember Intent: An If node checks if the user\u0027s chatInput starts with the word \"remember\".\nAuto Commit via Webhook: If the intent is to remember, an HTTPRequest node triggers the separate \"Commit to Memory\" workflow, passing the necessary IDs to save the new fact in the background.\nPhase 3: Main Logic Branching\nIf Rag is Active: This is the primary traffic controller. It checks if a rag_session_Id was provided.\nIf True: The workflow proceeds down the full RAG path to retrieve long-term memory.\nIf False: The workflow bypasses the RAG steps and proceeds directly to retrieve only the short-term conversational history.\nPhase 4: The RAG Path (Dual-Retrieval)\nThis path executes if RAG is active.\nCreate Query Embedding: An HTTPRequest node takes the user\u0027s chatInput and calls the OpenAI API to convert it into a vector embedding.\nFormat Data for Vector Search: A Code node prepares the data for the database search, formatting the embedding vector into the required string format for pgvector.\nParallel Retrieval: The workflow splits again to perform two simultaneous database lookups:\nRetrieve Committed Memory: A Postgres node searches the rag_store for memories from the live conversation.\nRetrieve RAG Chunks: A Postgres node searches the rag_store for memories from the initial bootstrapped knowledge base.\nMerge1: A Merge node combines the results from both retrieval steps into a single list.\nRAG Context consolidator: A Code node takes the merged list, removes any duplicate memories, and formats the unique results into a single, clean block of text.\nFormat History for AI: A Code node takes the consolidated text and wraps it in a standard { role: \u0027system\u0027, content: \u0027...\u0027 } object, ready for the final prompt.\nPhase 5: Final Prompt Assembly \u0026 AI Call (Three-Input Assembly)\nGet Recent history: (Runs in parallel to the RAG path) A Postgres node queries the conversation_history table for the last 6 turns of the conversation.\nFormat Recent History: A Code node formats these turns into the standard OpenAI message format.\nFormat Current Input: A Code node formats the user\u0027s current message into the standard format, including optional image parts.\nBuild OpenAI Payload1: The final Merge node has been removed. This Code node now receives three inputs directly and deterministically: (Input 0) RAG context from \"Format History for AI\" (as system content); (Input 1) recent conversational history from \"Format Recent History\"; (Input 2) the current user input from \"Format Current Input\". It selects the system persona, concatenates any RAG context, appends history, then the current user message, and sets usedVision=true when an image part is present.\nHTTP Request1: Sends the final payload to the OpenAI Chat Completions API.\nPhase 6: Response and Finalization\nCode1: A Code node parses the response from OpenAI, extracts the AI\u0027s reply, and performs cleaning and truncation. It also checks for any status messages from the background Auto-Commit process and appends them to the reply.\nPostgres2 (Save AI Reply): Saves the AI\u0027s generated response back to the conversation_history table.\nRespond to Webhook2: Sends the final, clean reply back to the Chat 8 UI, completing the cycle.","height":80,"width":580},"type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[1460,460],"id":"360f303d-03af-4c7c-866e-59a2de8f0818","name":"Sticky Note"},{"parameters":{"operation":"executeQuery","query":"SELECT role, content\nFROM conversation_history\nWHERE session_id = $1\nORDER BY created_at DESC\nLIMIT 6; -- Fetch more (e.g., 3 user, 3 AI) to ensure we get a few full turns","options":{"queryReplacement":"={{ [$json.session_Id] }}"}},"type":"n8n-nodes-base.postgres","typeVersion":2.6,"position":[2500,1100],"id":"dbb96626-8940-4478-a076-77f5cf4a12e9","name":"Get Recent history","credentials":{"postgres":{"id":"e1ilQ03VC1lcdrt2","name":"Postgres account 2 for BobMemory"}}},{"parameters":{"jsCode":"// Code Node: Format Recent History\n// Input: items from \"Get Recent History\" node (array of {role, content} objects, newest first)\n// Output: A single item containing a \u0027recent_history\u0027 array, formatted for OpenAI (oldest first)\n\nconst inputItems = $input.all(); // Get all input items (each is a row from DB)\nlet formattedHistory = [];\n\n// Check if there are any input items\nif (inputItems.length \u003e 0) {\n    // Extract .json from each item to get the actual data rows\n    const historyRecords = inputItems.map(item =\u003e item.json); \n\n    if (historyRecords \u0026\u0026 historyRecords.length \u003e 0) {\n      // The history from DB is newest first (ORDER BY created_at DESC), \n      // so reverse it to get oldest first for the OpenAI prompt\n      formattedHistory = historyRecords.reverse().map(record =\u003e {\n        let roleToUse = \u0027unknown\u0027;\n        if (typeof record.role === \u0027string\u0027) {\n            const dbRole = record.role.toLowerCase();\n            if (dbRole === \u0027user\u0027) {\n                roleToUse = \u0027user\u0027;\n            } else if (dbRole === \u0027assistant\u0027 || dbRole === \u0027ai\u0027 || dbRole === \u0027model\u0027) {\n                roleToUse = \u0027assistant\u0027;\n            } else {\n                console.warn(`Format Recent History: Unknown role \u0027${dbRole}\u0027 found, mapping to \u0027user\u0027. Content: ${(record.content || \"\").substring(0,50)}`);\n                roleToUse = \u0027user\u0027; // Fallback for unknown roles\n            }\n        } else {\n             console.warn(`Format Recent History: Missing or invalid role for record. Defaulting to \u0027user\u0027. Record:`, record);\n             roleToUse = \u0027user\u0027;\n        }\n        const content = (typeof record.content === \u0027string\u0027) ? record.content : \u0027\u0027;\n        \n        return {\n          role: roleToUse, \n          content: content\n        };\n      });\n    }\n}\n\n// Output a single item, with the formatted history array in a property\nreturn formattedHistory.map(item =\u003e ({ json: item }));"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[2720,1100],"id":"daf48b82-446e-46f5-b637-c171fea6576c","name":"Format Recent History"},{"parameters":{"schema":{"__rl":true,"mode":"list","value":"public"},"table":{"__rl":true,"value":"conversation_history","mode":"list","cachedResultName":"conversation_history"},"columns":{"mappingMode":"defineBelow","value":{"session_id":"={{ $json.session_Id }}","role":"user","content":"={{ $json.chatInput }}"},"matchingColumns":["id"],"schema":[{"id":"id","displayName":"id","required":false,"defaultMatch":true,"display":true,"type":"number","canBeUsedToMatch":true,"removed":true},{"id":"session_id","displayName":"session_id","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true},{"id":"role","displayName":"role","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true},{"id":"content","displayName":"content","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true},{"id":"created_at","displayName":"created_at","required":false,"defaultMatch":false,"display":true,"type":"dateTime","canBeUsedToMatch":true,"removed":true}],"attemptToConvertTypes":false,"convertFieldsToString":false},"options":{}},"type":"n8n-nodes-base.postgres","typeVersion":2.6,"position":[960,640],"id":"b9ad4972-0fbb-44c3-bdbd-6b70671dccca","name":"Save User Message to History","credentials":{"postgres":{"id":"e1ilQ03VC1lcdrt2","name":"Postgres account 2 for BobMemory"}}},{"parameters":{"assignments":{"assignments":[{"id":"ecc57f20-7f83-4ed7-bc40-a63bf0b318b7","name":"session_Id","value":"={{ $(\u0027Get Session ID \u0026 Input\u0027).item.json.session_Id }}","type":"string"},{"id":"1454ec7a-35ff-4bf1-aaf6-0bd559c6ddca","name":"chatInput","value":"={{ $(\u0027Get Session ID \u0026 Input\u0027).item.json.chatInput }}","type":"string"},{"id":"e45f0fda-84cf-43e7-94ff-41ed896ce7e8","name":"=imageData","value":"={{ $(\"Get Session ID \u0026 Input\").item.json.imageData }}","type":"string"},{"id":"09c1b52a-2cf0-4101-8a06-8ae66e1d8ea9","name":"rag_session_Id","value":"={{ $(\u0027Get Session ID \u0026 Input\u0027).item.json.rag_session_Id }}","type":"string"},{"id":"5c53893f-4d94-4559-bef2-dc3123f2ae94","name":"project_Id","value":"={{ $(\u0027Get Session ID \u0026 Input\u0027).item.json.project_Id }}","type":"string"}]},"options":{}},"type":"n8n-nodes-base.set","typeVersion":3.4,"position":[1180,640],"id":"25c45ac5-a24e-4b5f-9395-c347793b5d3b","name":"Preserve Current Inputs"},{"parameters":{"assignments":{"assignments":[{"id":"b6a2f6aa-4b9d-4a9e-b0e3-6e1e57e7f4f1","name":"autoCommitEnabled","value":"true","type":"boolean"}]},"options":{}},"type":"n8n-nodes-base.set","typeVersion":3.4,"position":[1400,620],"id":"c51b9d38-dc54-461d-a6b4-55277cf80ae2","name":"Config: Auto-Commit Enabled"},{"parameters":{"conditions":{"options":{"caseSensitive":false,"leftValue":"","typeValidation":"strict","version":2},"conditions":[{"id":"9b2a5e2c-30f2-4e2e-9e0d-7fbf5c7b8bd1","leftValue":"={{ $json.chatInput }}","rightValue":"^\\s*remember\\b","operator":{"type":"string","operation":"regex","singleValue":true}},{"id":"f4d0a1b2-5e6f-4a7b-8c9d-1e2f3a4b5c6d","leftValue":"={{ $(\u0027Config: Auto-Commit Enabled\u0027).first().json.autoCommitEnabled }}","rightValue":true,"operator":{"type":"boolean","operation":"equals"}}],"combinator":"and"},"options":{}},"type":"n8n-nodes-base.if","typeVersion":2.2,"position":[1620,620],"id":"ed459c71-7a55-4a47-9c59-86bece5a27ec","name":"If Remember Intent"},{"parameters":{"method":"POST","url":"https://mhcmike.app.n8n.cloud/webhook/6c1ce608-2f7a-457b-9afc-f0be5ef4bd4c","sendHeaders":true,"headerParameters":{"parameters":[{"name":"Content-Type","value":"application/json"}]},"sendBody":true,"specifyBody":"json","jsonBody":"={{ { chat_session_id: $(\u0027Preserve Current Inputs\u0027).first().json.session_Id, rag_session_id: $(\u0027Preserve Current Inputs\u0027).first().json.rag_session_Id } }}","options":{"response":{"response":{"responseFormat":"json"}}}},"type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[1840,620],"id":"49d09a57-0895-4c57-b876-6f57ec135914","name":"Auto Commit via Webhook"},{"parameters":{"schema":{"__rl":true,"mode":"list","value":"public"},"table":{"__rl":true,"value":"conversation_history","mode":"list","cachedResultName":"conversation_history"},"columns":{"mappingMode":"defineBelow","value":{"session_id":"={{ $(\u0027Preserve Current Inputs\u0027).first().json.session_Id }}","role":"\u0027assistant\u0027","content":"={{ $json.reply }}"},"matchingColumns":["id"],"schema":[{"id":"id","displayName":"id","required":false,"defaultMatch":true,"display":true,"type":"number","canBeUsedToMatch":true,"removed":true},{"id":"session_id","displayName":"session_id","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true},{"id":"role","displayName":"role","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true},{"id":"content","displayName":"content","required":false,"defaultMatch":false,"display":true,"type":"string","canBeUsedToMatch":true},{"id":"created_at","displayName":"created_at","required":false,"defaultMatch":false,"display":true,"type":"dateTime","canBeUsedToMatch":true,"removed":true}],"attemptToConvertTypes":false,"convertFieldsToString":false},"options":{}},"type":"n8n-nodes-base.postgres","typeVersion":2.6,"position":[3820,700],"id":"90097831-9ca8-4d28-b6ae-5112389cd548","name":"Postgres2 (Save AI Reply)","alwaysOutputData":true,"credentials":{"postgres":{"id":"e1ilQ03VC1lcdrt2","name":"Postgres account 2 for BobMemory"}}},{"parameters":{"numberInputs":3},"type":"n8n-nodes-base.merge","typeVersion":3.1,"position":[2940,900],"id":"c0042ce4-1e85-484d-a51d-3dc62c2065f2","name":"Merge"},{"parameters":{"jsCode":"// Get all items from all inputs\nconst allItems = $input.all();\n\n// Gracefully handle cases where one or both searches return no results.\nif (allItems.length === 0) {\n  return [{ json: { combined_rag_context: \"No relevant context was found in the knowledge base.\" } }];\n}\n\n// Use a Set to automatically handle duplicates\nconst uniqueContent = new Set();\nallItems.forEach(item =\u003e {\n  if (item.json.original_content) {\n    uniqueContent.add(item.json.original_content);\n  }\n});\n\n// Join the unique snippets together\nconst combinedText = Array.from(uniqueContent).join(\u0027\\n---\\n\u0027);\n\nreturn [{\n  json: {\n    combined_rag_context: combinedText\n  }\n}];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[2500,820],"id":"d0ad70ab-6a02-4297-bd7a-779c9ba4ace0","name":"RAG Context consolidator"},{"parameters":{"jsCode":"// The incoming data from the webhook has the user\u0027s text in the \u0027chatInput\u0027 field.\nconst userMessage = $input.item.json.chatInput;\n\n// Check if the user\u0027s message was successfully found.\nif (userMessage === undefined) {\n  // If not found, log the incoming data for debugging and throw a clear error.\n  console.log(\"ERROR: Could not find \u0027chatInput\u0027 in the input data:\", JSON.stringify($input.item, null, 2));\n  throw new Error(\"The user\u0027s message (chatInput) was not found in the input from the webhook.\");\n}\n\n// If the message is found, return it in the standard format for the chat model.\nreturn [{\n  json: {\n    role: \u0027user\u0027,\n    content: userMessage\n  }\n}];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[2720,440],"id":"20cad6d2-28a5-4cab-aa71-9c6242016e85","name":"Format Current Input"},{"parameters":{"jsCode":"// --- Configuration ---\nconst OPENAI_MODEL_NAME = \u0027gpt-5\u0027;\n\n// Minimal, safe persona handling: prefer explicit system_prompt_content from the webhook;\n// otherwise allow a small, hard-coded allowlist via persona_key; else default to \u0027bob\u0027.\nconst PERSONAS = {\n  bob: `You are Bob, a helpful and knowledgeable AI assistant for Mike Holland. Your goal is to provide accurate and relevant answers.\n\nMemory \u0026 Privacy Policy (for this project UI):\n- You ARE allowed to remember non-sensitive project-related details the user asks you to remember (e.g., addresses, codes, preferences) for use within this project.\n- When the user says \"remember\" or similar, acknowledge positively and briefly restate the fact.\n- Clarify, only when relevant, that permanent persistence across restarts requires pressing the \"Commit to Memory\" button in the UI. Until then, you\u0027ll remember it in this session.\n- If the user explicitly marks something as private/sensitive (e.g., SSN, passwords), decline to store and suggest safer alternatives.\n\nAnswering Guidance:\n1. Prioritize the Knowledge Base: First, check the provided context from the knowledge base. If it directly answers the user\u0027s question, use it.\n2. Use General Knowledge if the KB lacks details.\n3. Be Conversational and concise; no need to mention internal mechanics unless helpful.\n4. Maintain Persona: A confident, direct \"Badda Boom Badda Bing\" style.`,\n  coach: `You are a supportive coach. Be encouraging, concise, and practical. Offer a brief plan with up to three steps and ask at most one clarifying question when essential.`,\n  pm: `You are a pragmatic project manager. Be structured, risk-aware, and end with a short list of next steps and owners when applicable.`\n};\n\nfunction pickSystemPrompt() {\n  try {\n    const body = $(\u0027Webhook1\u0027).first()?.json?.body || {};\n    const direct = (typeof body.system_prompt_content === \u0027string\u0027) ? body.system_prompt_content.trim() : \u0027\u0027;\n    if (direct) return direct;\n    const key = (body.persona_key || \u0027\u0027).toString().toLowerCase();\n    if (PERSONAS[key]) return PERSONAS[key];\n  } catch (e) { /* fall through to default */ }\n  return PERSONAS.bob;\n}\n\nconst SYSTEM_PROMPT_CONTENT = pickSystemPrompt();\n\n// --- Main Logic ---\n// 1. Get the user\u0027s message and any RAG context from the previous step.\nconst incomingItems = $input.all().map(item =\u003e item.json);\n\n// 2. Separate the RAG context from the user\u0027s actual message history.\nconst ragContext = incomingItems.filter(message =\u003e message.role === \u0027system\u0027).map(item =\u003e item.content).join(\u0027\\n\\n\u0027);\nconst messageHistory = incomingItems.filter(message =\u003e message.role !== \u0027system\u0027);\n\n// 3. Build the final array of messages for the API call.\nconst finalMessages = [];\n\n// 4. ALWAYS start with our main system prompt.\nlet finalSystemPrompt = SYSTEM_PROMPT_CONTENT;\n\n// 5. If RAG context was found, append it to the system prompt.\nif (ragContext) {\n  finalSystemPrompt += `\\n\\n--- Relevant Knowledge Base Context ---\\n${ragContext}`;\n  console.log(\u0027RAG context found and appended to system prompt.\u0027);\n} else {\n  console.log(\u0027No RAG context found. Using default or selected persona prompt.\u0027);\n}\n\n// Add the complete system prompt as the first message.\nfinalMessages.push({ role: \u0027system\u0027, content: finalSystemPrompt });\n\n// Add the rest of the message history (the user\u0027s actual conversation).\nfinalMessages.push(...messageHistory);\n\n// 6. Build the final payload object.\nconst openAIPayload = { model: OPENAI_MODEL_NAME, messages: finalMessages };\nconsole.log(\u0027Final OpenAI Payload to be sent:\u0027, JSON.stringify(openAIPayload, null, 2));\n\n// 7. Return the final payload.\nreturn [{ json: openAIPayload }];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[3160,900],"id":"19594ec4-30c7-429b-929f-8caeff5bc720","name":"Build OpenAI Payload1"},{"parameters":{"options":{}},"type":"n8n-nodes-base.respondToWebhook","typeVersion":1.2,"position":[740,440],"id":"45317fc5-90ae-4116-9270-739338a99081","name":"Pre-flight Request"},{"parameters":{"conditions":{"options":{"caseSensitive":true,"leftValue":"","typeValidation":"strict","version":2},"conditions":[{"id":"4b3cfa86-fdf5-4632-bbd3-84bb73ffb9bf","leftValue":"={{ $json.rag_session_Id }}","rightValue":"","operator":{"type":"string","operation":"notEmpty","singleValue":true}}],"combinator":"and"},"options":{}},"type":"n8n-nodes-base.if","typeVersion":2.2,"position":[1400,1000],"id":"3cba10c1-60fc-4f84-b32f-2cd882ba0202","name":"If Rag is Active"},{"parameters":{"operation":"executeQuery","query":"SELECT\n  original_content,\n  role,\n  embedding \u003c=\u003e $3::vector AS distance\nFROM\n  rag_store\nWHERE\n  project_id = $1 AND session_id = $2\nORDER BY\n  distance ASC\nLIMIT 3","options":{"queryReplacement":"=[\n  {{ $(\u0027Webhook1\u0027).first().json.body.project_id }},\n  {{ $(\u0027Format Data for Vector Search\u0027).first().json.current_session_id_for_search }},\n  {{ $(\u0027Format Data for Vector Search\u0027).first().json.query_embedding_for_pg }}\n]"}},"type":"n8n-nodes-base.postgres","typeVersion":2.6,"position":[2060,720],"id":"647dfce4-7697-47c8-80dc-3d78c4d79258","name":"Retrieve Committed Memory","credentials":{"postgres":{"id":"e1ilQ03VC1lcdrt2","name":"Postgres account 2 for BobMemory"}}},{"parameters":{"operation":"executeQuery","query":"SELECT\n  original_content,\n  role,\n  embedding \u003c=\u003e $3::vector AS distance\nFROM\n  rag_store\nWHERE\n  project_id = $1 AND session_id = $2\nORDER BY\n  distance ASC\nLIMIT 3","options":{"queryReplacement":"=[\n  {{ $(\u0027Preserve Current Inputs\u0027).first().json.project_Id }},\n  {{ $(\u0027Format Data for Vector Search\u0027).first().json.current_session_id_for_search }},\n  {{ $(\u0027Format Data for Vector Search\u0027).first().json.query_embedding_for_pg }}\n]"}},"type":"n8n-nodes-base.postgres","typeVersion":2.6,"position":[2060,920],"id":"e538bf0a-72fc-4570-8245-94f546682cd6","name":"Retrieve RAG Chunks","credentials":{"postgres":{"id":"e1ilQ03VC1lcdrt2","name":"Postgres account 2 for BobMemory"}}},{"parameters":{},"type":"n8n-nodes-base.merge","typeVersion":3.1,"position":[2280,820],"id":"571eaa9a-4dc3-4c21-991d-0fffd94319ad","name":"Merge1"},{"parameters":{"respondWith":"json","responseBody":"={{ $json }}","options":{"responseHeaders":{"entries":[{"name":"Access-Control-Allow-Origin","value":"*"}]}}},"type":"n8n-nodes-base.respondToWebhook","typeVersion":1.2,"position":[3820,900],"id":"de5257a7-642a-4eeb-81cb-387d52357d28","name":"Respond to Webhook2"},{"parameters":{"multipleMethods":true,"path":"3c92075f-a856-439a-b70d-73f3c847f8fa","responseMode":"responseNode","options":{"responseHeaders":{"entries":[{"name":"Access-Control-Allow-Methods","value":"GET, POST, OPTIONS"},{"name":"Access-Control-Allow-Headers","value":"Content-Type, Authorization"},{"name":"Access-Control-Allow-Origin","value":"*"}]}}},"type":"n8n-nodes-base.webhook","typeVersion":2,"position":[520,520],"id":"569f912a-4714-4403-9541-e09cd739965a","name":"Webhook1","webhookId":"58378ca2-9b32-4dfc-9e25-88b4fdce6ed3","options":{"allowedOrigins":"*","responseHeaders":{"entries":[{"name":"Access-Control-Allow-Methods","value":"GET, POST, OPTIONS"},{"name":"Access-Control-Allow-Headers","value":"Content-Type, Authorization"},{"name":"Access-Control-Allow-Origin","value":"*"}]}},"notes":"update 2"}],"pinData":{"Webhook1":[]},"connections":{"HTTP Request1":{"main":[[{"node":"Code1","type":"main","index":0}]]},"Code1":{"main":[[{"node":"Postgres2 (Save AI Reply)","type":"main","index":0},{"node":"Respond to Webhook2","type":"main","index":0}]]},"Format History for AI":{"main":[[{"node":"Merge","type":"main","index":0}]]},"Create Query Embedding":{"main":[[{"node":"Format Data for Vector Search","type":"main","index":0}]]},"Format Data for Vector Search":{"main":[[{"node":"Retrieve Committed Memory","type":"main","index":0},{"node":"Retrieve RAG Chunks","type":"main","index":0}]]},"Get Session ID \u0026 Input":{"main":[[{"node":"Save User Message to History","type":"main","index":0}]]},"Get Recent history":{"main":[[{"node":"Format Recent History","type":"main","index":0}]]},"Format Recent History":{"main":[[{"node":"Merge","type":"main","index":1}]]},"Save User Message to History":{"main":[[{"node":"Preserve Current Inputs","type":"main","index":0}]]},"Preserve Current Inputs":{"main":[[{"node":"Format Current Input","type":"main","index":0},{"node":"If Rag is Active","type":"main","index":0},{"node":"Config: Auto-Commit Enabled","type":"main","index":0}]]},"Config: Auto-Commit Enabled":{"main":[[{"node":"If Remember Intent","type":"main","index":0}]]},"If Remember Intent":{"main":[[{"node":"Auto Commit via Webhook","type":"main","index":0}]]},"Merge":{"main":[[{"node":"Build OpenAI Payload1","type":"main","index":0}]]},"RAG Context consolidator":{"main":[[{"node":"Format History for AI","type":"main","index":0}]]},"Format Current Input":{"main":[[{"node":"Merge","type":"main","index":2}]]},"Build OpenAI Payload1":{"main":[[{"node":"HTTP Request1","type":"main","index":0}]]},"If Rag is Active":{"main":[[{"node":"Create Query Embedding","type":"main","index":0},{"node":"Get Recent history","type":"main","index":0}],[{"node":"Get Recent history","type":"main","index":0}]]},"Retrieve Committed Memory":{"main":[[{"node":"Merge1","type":"main","index":0}]]},"Retrieve RAG Chunks":{"main":[[{"node":"Merge1","type":"main","index":1}]]},"Merge1":{"main":[[{"node":"RAG Context consolidator","type":"main","index":0}]]},"Webhook1":{"main":[[{"node":"Pre-flight Request","type":"main","index":0}],[{"node":"Get Session ID \u0026 Input","type":"main","index":0}]]}},"active":false,"settings":{"executionOrder":"v1"},"versionId":"2ac768b4-8b48-417b-9c47-0fcc0a3f7870","meta":{"instanceId":"c4c30886ead33627446590bc73a5bef82db63d1121e51ff9b9b6f6ea92a27ca3"},"id":"GR9fNPJS0bGgjoTg","tags":[]}
